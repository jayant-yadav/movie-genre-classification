{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V0MlomrGAhhg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./Movie_Poster_Metadata/groundtruth\"\n",
    "temp_path = \"./Movie_Poster_Metadata/temp_groundtruth\"\n",
    "path2 = \"./Movie_Poster_Metadata/updated_groundtruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input file and creating a clean one\n",
    "Note: only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(path1)\n",
    " \n",
    "if not os.path.exists(temp_path):\n",
    "  os.makedirs(temp_path)    \n",
    "\n",
    "if not os.path.exists(path2):\n",
    "  os.makedirs(path2)\n",
    "\n",
    "\n",
    "\n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(path1+'/'+file_name,'r',encoding='utf-16-le') as file1:\n",
    "\n",
    "        temp_file = open(temp_path+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        for line in file1.readlines():\n",
    "\n",
    "            line = line.replace(\"}\\n\",\"},\\n\")\n",
    "            \n",
    "            # reading all lines that begin with \"  \"_id\"\"\n",
    "            y = re.findall(\"^  \\\"_id\\\"\", line)\n",
    "            if not y:\n",
    "                temp_file.write(line)\n",
    "\n",
    "    file1.close()\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(temp_path)\n",
    " \n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(temp_path+'/'+file_name,'r',encoding='utf-8') as temp_file:\n",
    "    \n",
    "        file2 = open(path2+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        lines = temp_file.readlines()\n",
    "        lines = lines[1:-1]\n",
    "\n",
    "        file2.write(\"[{\")\n",
    "        file2.writelines(lines)\n",
    "        file2.write(\"}]\")\n",
    "        \n",
    "    temp_file.close()\n",
    "    file2.close()\n",
    "\n",
    "shutil.rmtree(temp_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the data set\n",
    "Note: only run once\n",
    "\n",
    "To-Do: Balance data according to occurence of genres. Summarize genres with little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = \"./Movie_Poster_Dataset\"\n",
    "\n",
    "# Going through all jpg-files, they are chopped up into 100x100 chunks and saved into a new folder\n",
    "for dirname in os.listdir(path3):\n",
    "    for filename in os.listdir(path3 + \"/\" + dirname):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if(ext == '.jpg'):\n",
    "            image = Image.open(os.path.join(path3 + \"/\" + dirname, filename))\n",
    "            width, height = image.size\n",
    "            chopsize = 100\n",
    "            for x0 in range(0, width, chopsize):\n",
    "                for y0 in range(0, height, chopsize):\n",
    "                    if(y0+chopsize <= height and x0+chopsize <= width):\n",
    "                        box = (x0, y0, x0+chopsize, y0+chopsize)\n",
    "                        image.crop(box).save('./Movie_Poster_Dataset_Cropped/%s.x%03d.y%03d.jpg' % (filename.replace('.jpg',''), x0, y0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = \"./Movie_Poster_Dataset\"\n",
    "\n",
    "# Going through all jpg-files, they are chopped up into 100x100 chunks and saved into a new folder\n",
    "for dirname in os.listdir(path3):\n",
    "    for filename in os.listdir(path3 + \"/\" + dirname):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if(ext == '.jpg'):\n",
    "            image = Image.open(os.path.join(path3 + \"/\" + dirname, filename))\n",
    "            box = (0, 0, 100, 100)\n",
    "            image.crop(box).save('./Movie_Poster_Dataset_Cropped_Once/%s.jpg' % (filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be used later to augment data of underrepresented genres (balance data)\n",
    "\n",
    "print('Nr of movies in json: '+str(len(dicts)))\n",
    "missing = []\n",
    "for obj in dicts:\n",
    "    genrelist = obj.get('Genre').split(',')\n",
    "    fname = obj.get('imdbID') + '.jpg'\n",
    "    if(path.exists(fname)):\n",
    "        for genre in genrelist:\n",
    "            #copy the file with name obj.key(\"imdbID\") to each genre folder\n",
    "            if(genre == 'N/A'):\n",
    "                shutil.copy2(os.path.join('.', fname), './NotApplicable')\n",
    "            elif(genre == 'Adult' || genre == 'Game-Show' || genre == 'News' || genre == 'Reality-TV' || genre == 'Talk-Show' || genre == 'Western'):\n",
    "                shutil.copy2(os.path.join('.', fname), './Other')\n",
    "            else:\n",
    "                shutil.copy2(os.path.join('.', fname), './'+genre.lstrip())\n",
    "    else:\n",
    "        missing.append(fname)\n",
    "\n",
    "\n",
    "print('Nr of missing IDs: '+str(len(missing)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to append all the json objects into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbID        object\n",
      "Director      object\n",
      "Genre         object\n",
      "imdbRating    object\n",
      "dtype: object\n",
      "       imdbID                                  Director  \\\n",
      "0   tt0080684                            Irvin Kershner   \n",
      "1   tt0081562                            Sidney Poitier   \n",
      "2   tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3   tt0080377                            Buddy Van Horn   \n",
      "4   tt0081375                              Howard Zieff   \n",
      "5   tt0080549                             Michael Apted   \n",
      "6   tt0081529                               Hal Needham   \n",
      "7   tt0080453                            Randal Kleiser   \n",
      "8   tt0080455                               John Landis   \n",
      "9   tt0081283                            Robert Redford   \n",
      "10  tt0081353                             Robert Altman   \n",
      "11  tt0081696                             James Bridges   \n",
      "12  tt0081505                           Stanley Kubrick   \n",
      "13  tt0081480                              Jay Sandrich   \n",
      "14  tt0080520                               Tommy Chong   \n",
      "15  tt0080487                              Harold Ramis   \n",
      "16  tt0080474                          Stuart Rosenberg   \n",
      "17  tt0081060                               Ron Maxwell   \n",
      "18  tt0080661                            Brian De Palma   \n",
      "19  tt0080948                         Richard Fleischer   \n",
      "\n",
      "                         Genre imdbRating  \n",
      "0   Action, Adventure, Fantasy        8.8  \n",
      "1                Comedy, Crime        6.8  \n",
      "2                       Comedy        7.8  \n",
      "3               Action, Comedy        6.0  \n",
      "4                  Comedy, War        6.1  \n",
      "5      Biography, Drama, Music        7.5  \n",
      "6               Action, Comedy        5.1  \n",
      "7    Adventure, Drama, Romance        5.7  \n",
      "8        Action, Comedy, Crime        7.9  \n",
      "9                        Drama        7.8  \n",
      "10   Adventure, Comedy, Family        5.2  \n",
      "11     Drama, Romance, Western        6.2  \n",
      "12               Drama, Horror        8.4  \n",
      "13             Comedy, Romance        6.7  \n",
      "14       Comedy, Crime, Sci-Fi        6.1  \n",
      "15               Comedy, Sport        7.4  \n",
      "16                       Drama        7.1  \n",
      "17               Comedy, Drama        6.4  \n",
      "18           Mystery, Thriller        7.1  \n",
      "19       Drama, Music, Romance        5.7  \n",
      "(8873, 4)\n"
     ]
    }
   ],
   "source": [
    "dir_list = os.listdir(path2)\n",
    "\n",
    "movies_df = pd.DataFrame()\n",
    "\n",
    "for file_name in dir_list:    \n",
    "\n",
    "#     try:\n",
    "    df = pd.read_json(path2+'/'+file_name,encoding='utf-8',orient='records')\n",
    "    df = df[['imdbID','Director','Genre','imdbRating']]\n",
    "    movies_df = pd.concat([movies_df,df], ignore_index=True)\n",
    "\n",
    "#     except:\n",
    "#         print(file_name)\n",
    "        \n",
    "print(movies_df.dtypes)\n",
    "print(movies_df.head(20))\n",
    "print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi-hot encoded genre vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates and set imdbID as index\n",
    "movies_df = movies_df.drop_duplicates(subset=[\"imdbID\"], keep=\"last\")\n",
    "movies_df.set_index(\"imdbID\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action' 'Adult' 'Adventure' 'Animation' 'Biography' 'Comedy' 'Crime'\n",
      " 'Documentary' 'Drama' 'Family' 'Fantasy' 'Game-Show' 'History' 'Horror'\n",
      " 'Music' 'Musical' 'Mystery' 'N/A' 'News' 'Reality-TV' 'Romance' 'Sci-Fi'\n",
      " 'Short' 'Sport' 'Talk-Show' 'Thriller' 'War' 'Western']\n",
      "                                           Director  \\\n",
      "imdbID                                                \n",
      "tt0080684                            Irvin Kershner   \n",
      "tt0081562                            Sidney Poitier   \n",
      "tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "tt0080377                            Buddy Van Horn   \n",
      "tt0081375                              Howard Zieff   \n",
      "tt0080549                             Michael Apted   \n",
      "tt0081529                               Hal Needham   \n",
      "tt0080453                            Randal Kleiser   \n",
      "tt0080455                               John Landis   \n",
      "tt0081283                            Robert Redford   \n",
      "\n",
      "                                Genre imdbRating  \\\n",
      "imdbID                                             \n",
      "tt0080684  Action, Adventure, Fantasy        8.8   \n",
      "tt0081562               Comedy, Crime        6.8   \n",
      "tt0080339                      Comedy        7.8   \n",
      "tt0080377              Action, Comedy        6.0   \n",
      "tt0081375                 Comedy, War        6.1   \n",
      "tt0080549     Biography, Drama, Music        7.5   \n",
      "tt0081529              Action, Comedy        5.1   \n",
      "tt0080453   Adventure, Drama, Romance        5.7   \n",
      "tt0080455       Action, Comedy, Crime        7.9   \n",
      "tt0081283                       Drama        7.8   \n",
      "\n",
      "                                                    multihot  \n",
      "imdbID                                                        \n",
      "tt0080684  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0081562  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0080339  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0080377  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0081375  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0080549  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0081529  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0080453  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0080455  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n",
      "tt0081283  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  \n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "multihot = mlb.fit_transform(movies_df[\"Genre\"].dropna().str.split(\", \"))\n",
    "genres_df = pd.DataFrame({\"multihot\":[multihot.astype(int)]}, index = movies_df.index)\n",
    "movies_df = pd.concat([movies_df, genres_df], axis=1 )\n",
    "print(mlb.classes_)\n",
    "print(movies_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary with multi-hot encoded vectors; index = imdbID\n",
    "multihot_dict = {movies_df.index.tolist()[i] : multihot[i] for i in range(0, len(multihot))}\n",
    "#print(multihot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the images to the dataframe\n",
    "Note: not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Director  \\\n",
      "imdbID                                                \n",
      "tt0080684                            Irvin Kershner   \n",
      "tt0081562                            Sidney Poitier   \n",
      "tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "tt0080377                            Buddy Van Horn   \n",
      "tt0081375                              Howard Zieff   \n",
      "tt0080549                             Michael Apted   \n",
      "tt0081529                               Hal Needham   \n",
      "tt0080453                            Randal Kleiser   \n",
      "tt0080455                               John Landis   \n",
      "tt0081283                            Robert Redford   \n",
      "\n",
      "                                Genre imdbRating  \\\n",
      "imdbID                                             \n",
      "tt0080684  Action, Adventure, Fantasy        8.8   \n",
      "tt0081562               Comedy, Crime        6.8   \n",
      "tt0080339                      Comedy        7.8   \n",
      "tt0080377              Action, Comedy        6.0   \n",
      "tt0081375                 Comedy, War        6.1   \n",
      "tt0080549     Biography, Drama, Music        7.5   \n",
      "tt0081529              Action, Comedy        5.1   \n",
      "tt0080453   Adventure, Drama, Romance        5.7   \n",
      "tt0080455       Action, Comedy, Crime        7.9   \n",
      "tt0081283                       Drama        7.8   \n",
      "\n",
      "                                                    multihot  \\\n",
      "imdbID                                                         \n",
      "tt0080684  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0081562  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0080339  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0080377  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0081375  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0080549  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0081529  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0080453  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0080455  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "tt0081283  [[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...   \n",
      "\n",
      "                                                      Poster  \\\n",
      "imdbID                                                         \n",
      "tt0080684  [[[255, 255, 255, 255, 255, 255, 255, 255, 254...   \n",
      "tt0081562  [[[253, 254, 255, 255, 255, 255, 255, 255, 254...   \n",
      "tt0080339  [[[237, 238, 238, 238, 238, 238, 239, 239, 239...   \n",
      "tt0080377  [[[102, 93, 117, 85, 98, 96, 93, 101, 95, 92, ...   \n",
      "tt0081375  [[[91, 92, 95, 96, 97, 98, 100, 101, 105, 103,...   \n",
      "tt0080549  [[[43, 43, 43, 43, 43, 42, 41, 41, 38, 37, 37,...   \n",
      "tt0081529  [[[98, 117, 88, 90, 96, 90, 79, 69, 68, 66, 67...   \n",
      "tt0080453  [[[122, 129, 103, 64, 60, 40, 49, 56, 64, 67, ...   \n",
      "tt0080455  [[[138, 138, 138, 138, 138, 138, 138, 138, 138...   \n",
      "tt0081283  [[[4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4...   \n",
      "\n",
      "                                                      Poster  \n",
      "imdbID                                                        \n",
      "tt0080684  [[[255, 255, 255, 255, 255, 255, 255, 255, 254...  \n",
      "tt0081562  [[[253, 254, 255, 255, 255, 255, 255, 255, 254...  \n",
      "tt0080339  [[[237, 238, 238, 238, 238, 238, 239, 239, 239...  \n",
      "tt0080377  [[[102, 93, 117, 85, 98, 96, 93, 101, 95, 92, ...  \n",
      "tt0081375  [[[91, 92, 95, 96, 97, 98, 100, 101, 105, 103,...  \n",
      "tt0080549  [[[43, 43, 43, 43, 43, 42, 41, 41, 38, 37, 37,...  \n",
      "tt0081529  [[[98, 117, 88, 90, 96, 90, 79, 69, 68, 66, 67...  \n",
      "tt0080453  [[[122, 129, 103, 64, 60, 40, 49, 56, 64, 67, ...  \n",
      "tt0080455  [[[138, 138, 138, 138, 138, 138, 138, 138, 138...  \n",
      "tt0081283  [[[4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4...  \n"
     ]
    }
   ],
   "source": [
    "flist=glob.glob('./Movie_Poster_Dataset/*/*.jpg')\n",
    "\n",
    "imdb_id_arr = [\"0\" for a in range(len(flist))]\n",
    "image_arr = [\"0\" for a in range(len(flist))]\n",
    "index = 0\n",
    "\n",
    "for filename in flist:\n",
    "        \n",
    "    imdb_id = filename[filename.index(\"tt\"):filename.index(\".jpg\")]\n",
    "        \n",
    "    imdb_id_arr[index] = imdb_id\n",
    "                \n",
    "    img = np.array(cv2.imread(filename))\n",
    "    img = np.swapaxes(img, 2,0)\n",
    "    img = np.swapaxes(img, 2,1)\n",
    "    \n",
    "    image_arr[index] = img\n",
    "    \n",
    "    index +=1 \n",
    "        \n",
    "image_dict = {\n",
    "    \"imdbID\": imdb_id_arr,\n",
    "    \"Poster\": image_arr\n",
    "}\n",
    "\n",
    "images_df = pd.DataFrame.from_dict(image_dict)\n",
    "images_df = images_df.drop_duplicates(subset=[\"imdbID\"], keep=\"last\")\n",
    "images_df.set_index(\"imdbID\", inplace=True)\n",
    "movies_df = pd.concat([movies_df, images_df], axis=1)\n",
    "print(movies_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through a convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "dropout = [0.3, 0.3, 0.3, 0.3, 0.2, 0.2, 0.2, 0.2, 0.15]\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "#images need to have the same size!!\n",
    "flist=glob.glob('./Movie_Poster_Dataset_Cropped_Once/*.jpg')\n",
    "\n",
    "length=int(len(flist)*training_size)\n",
    "i = 0\n",
    "\n",
    "#create lists with input data (images) and output data (multi-hot encoded genre vectors)\n",
    "for filename in flist:\n",
    "        \n",
    "    imdb_id = filename[filename.index(\"tt\"):filename.index(\".jpg\")]\n",
    "      \n",
    "    if imdb_id in multihot_dict:\n",
    "        img = np.array(cv2.imread(filename))\n",
    "        img = np.swapaxes(img, 2,0)\n",
    "        img = np.swapaxes(img, 2,1)\n",
    "        \n",
    "        genre_arr = np.empty([28])\n",
    "\n",
    "        for j in range(len(multihot_dict[imdb_id])):\n",
    "            genre_arr[j] = multihot_dict[imdb_id][j]\n",
    "    \n",
    "        if(i<length):  \n",
    "            x_train.append(img)\n",
    "            y_train.append(genre_arr)\n",
    "        else:\n",
    "            x_test.append(img)\n",
    "            y_test.append(genre_arr)\n",
    "        \n",
    "        i +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5636\n",
      "5636\n",
      "[[[ 69   0   8 ... 202 202 202]\n",
      "  [ 73   8  17 ... 202 204 202]\n",
      "  [ 48   0   0 ... 206 208 206]\n",
      "  ...\n",
      "  [218 219 223 ... 152  98  85]\n",
      "  [216 217 222 ... 178  88  39]\n",
      "  [215 216 221 ... 202 101  23]]\n",
      "\n",
      " [[ 68   0   7 ... 232 232 232]\n",
      "  [ 72   7  16 ... 232 232 232]\n",
      "  [ 48   0   0 ... 234 233 234]\n",
      "  ...\n",
      "  [243 244 244 ... 157 103  90]\n",
      "  [241 242 243 ... 183  93  44]\n",
      "  [240 241 242 ... 207 106  28]]\n",
      "\n",
      " [[ 88  17  27 ... 233 233 233]\n",
      "  [ 92  27  36 ... 233 233 233]\n",
      "  [ 66  13  18 ... 235 235 235]\n",
      "  ...\n",
      "  [247 248 246 ... 166 112  99]\n",
      "  [245 246 245 ... 192 102  53]\n",
      "  [244 245 244 ... 216 115  37]]]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5636, 3, 100, 100)\n",
      "5636 train samples\n",
      "2416 test samples\n"
     ]
    }
   ],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=np.asarray(x_train,dtype=float)\n",
    "x_test=np.asarray(x_test,dtype=float)\n",
    "y_train=np.asarray(y_train,dtype=float)\n",
    "y_test=np.asarray(y_test,dtype=float)\n",
    "\n",
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv1_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv2): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv5): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv5_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (conv6): Conv2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv6_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3_drop): Dropout(p=0.15, inplace=False)\n",
      "  (fc4): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/5636 (0%)]\tLoss: 0.198342 0.347794\n",
      "Train Epoch: 0 [20/5636 (0%)]\tLoss: 0.178829 0.327386\n",
      "Train Epoch: 0 [40/5636 (1%)]\tLoss: 0.173594 0.318648\n",
      "Train Epoch: 0 [60/5636 (1%)]\tLoss: 0.164151 0.295207\n",
      "Train Epoch: 0 [80/5636 (1%)]\tLoss: 0.119921 0.200055\n",
      "Train Epoch: 0 [100/5636 (2%)]\tLoss: 0.268878 0.449286\n",
      "Train Epoch: 0 [120/5636 (2%)]\tLoss: 0.153481 0.245579\n",
      "Train Epoch: 0 [140/5636 (2%)]\tLoss: 0.131569 0.231223\n",
      "Train Epoch: 0 [160/5636 (3%)]\tLoss: 0.131470 0.207821\n",
      "Train Epoch: 0 [180/5636 (3%)]\tLoss: 0.108792 0.181644\n",
      "Train Epoch: 0 [200/5636 (4%)]\tLoss: 0.121097 0.210662\n",
      "Train Epoch: 0 [220/5636 (4%)]\tLoss: 0.123642 0.227612\n",
      "Train Epoch: 0 [240/5636 (4%)]\tLoss: 0.117712 0.204636\n",
      "Train Epoch: 0 [260/5636 (5%)]\tLoss: 0.111395 0.223657\n",
      "Train Epoch: 0 [280/5636 (5%)]\tLoss: 0.107988 0.166578\n",
      "Train Epoch: 0 [300/5636 (5%)]\tLoss: 0.102058 0.191011\n",
      "Train Epoch: 0 [320/5636 (6%)]\tLoss: 0.091572 0.172596\n",
      "Train Epoch: 0 [340/5636 (6%)]\tLoss: 0.097317 0.194834\n",
      "Train Epoch: 0 [360/5636 (6%)]\tLoss: 0.097183 0.178065\n",
      "Train Epoch: 0 [380/5636 (7%)]\tLoss: 0.107129 0.200649\n",
      "Train Epoch: 0 [400/5636 (7%)]\tLoss: 0.148356 0.244853\n",
      "Train Epoch: 0 [420/5636 (7%)]\tLoss: 0.111970 0.202161\n",
      "Train Epoch: 0 [440/5636 (8%)]\tLoss: 0.132306 0.251886\n",
      "Train Epoch: 0 [460/5636 (8%)]\tLoss: 0.112662 0.183955\n",
      "Train Epoch: 0 [480/5636 (9%)]\tLoss: 0.103716 0.191318\n",
      "Train Epoch: 0 [500/5636 (9%)]\tLoss: 0.123785 0.213967\n",
      "Train Epoch: 0 [520/5636 (9%)]\tLoss: 0.100910 0.158241\n",
      "Train Epoch: 0 [540/5636 (10%)]\tLoss: 0.120034 0.212558\n",
      "Train Epoch: 0 [560/5636 (10%)]\tLoss: 0.105582 0.173639\n",
      "Train Epoch: 0 [580/5636 (10%)]\tLoss: 0.118563 0.210439\n",
      "Train Epoch: 0 [600/5636 (11%)]\tLoss: 0.106419 0.222352\n",
      "Train Epoch: 0 [620/5636 (11%)]\tLoss: 0.095312 0.176576\n",
      "Train Epoch: 0 [640/5636 (11%)]\tLoss: 0.106557 0.189150\n",
      "Train Epoch: 0 [660/5636 (12%)]\tLoss: 0.092327 0.179103\n",
      "Train Epoch: 0 [680/5636 (12%)]\tLoss: 0.114057 0.206437\n",
      "Train Epoch: 0 [700/5636 (12%)]\tLoss: 0.110008 0.187556\n",
      "Train Epoch: 0 [720/5636 (13%)]\tLoss: 0.097818 0.171364\n",
      "Train Epoch: 0 [740/5636 (13%)]\tLoss: 0.107816 0.221481\n",
      "Train Epoch: 0 [760/5636 (13%)]\tLoss: 0.098231 0.158910\n",
      "Train Epoch: 0 [780/5636 (14%)]\tLoss: 0.088046 0.183570\n",
      "Train Epoch: 0 [800/5636 (14%)]\tLoss: 0.105313 0.181150\n",
      "Train Epoch: 0 [820/5636 (15%)]\tLoss: 0.105535 0.205064\n",
      "Train Epoch: 0 [840/5636 (15%)]\tLoss: 0.114806 0.191193\n",
      "Train Epoch: 0 [860/5636 (15%)]\tLoss: 0.090172 0.164747\n",
      "Train Epoch: 0 [880/5636 (16%)]\tLoss: 0.099604 0.172797\n",
      "Train Epoch: 0 [900/5636 (16%)]\tLoss: 0.081948 0.139463\n",
      "Train Epoch: 0 [920/5636 (16%)]\tLoss: 0.091715 0.161831\n",
      "Train Epoch: 0 [940/5636 (17%)]\tLoss: 0.106746 0.170847\n",
      "Train Epoch: 0 [960/5636 (17%)]\tLoss: 0.097798 0.201514\n",
      "Train Epoch: 0 [980/5636 (17%)]\tLoss: 0.099002 0.169003\n",
      "Train Epoch: 0 [1000/5636 (18%)]\tLoss: 0.092773 0.165255\n",
      "Train Epoch: 0 [1020/5636 (18%)]\tLoss: 0.094343 0.208450\n",
      "Train Epoch: 0 [1040/5636 (18%)]\tLoss: 0.109209 0.156263\n",
      "Train Epoch: 0 [1060/5636 (19%)]\tLoss: 0.082026 0.140211\n",
      "Train Epoch: 0 [1080/5636 (19%)]\tLoss: 0.084366 0.139187\n",
      "Train Epoch: 0 [1100/5636 (20%)]\tLoss: 0.097787 0.154869\n",
      "Train Epoch: 0 [1120/5636 (20%)]\tLoss: 0.090480 0.163014\n",
      "Train Epoch: 0 [1140/5636 (20%)]\tLoss: 0.092802 0.156165\n",
      "Train Epoch: 0 [1160/5636 (21%)]\tLoss: 0.089468 0.137813\n",
      "Train Epoch: 0 [1180/5636 (21%)]\tLoss: 0.095520 0.206913\n",
      "Train Epoch: 0 [1200/5636 (21%)]\tLoss: 0.095897 0.149653\n",
      "Train Epoch: 0 [1220/5636 (22%)]\tLoss: 0.090278 0.171979\n",
      "Train Epoch: 0 [1240/5636 (22%)]\tLoss: 0.102391 0.150907\n",
      "Train Epoch: 0 [1260/5636 (22%)]\tLoss: 0.093939 0.178104\n",
      "Train Epoch: 0 [1280/5636 (23%)]\tLoss: 0.098621 0.191976\n",
      "Train Epoch: 0 [1300/5636 (23%)]\tLoss: 0.089460 0.124237\n",
      "Train Epoch: 0 [1320/5636 (23%)]\tLoss: 0.080607 0.147062\n",
      "Train Epoch: 0 [1340/5636 (24%)]\tLoss: 0.099433 0.164271\n",
      "Train Epoch: 0 [1360/5636 (24%)]\tLoss: 0.096020 0.203217\n",
      "Train Epoch: 0 [1380/5636 (24%)]\tLoss: 0.082721 0.142009\n",
      "Train Epoch: 0 [1400/5636 (25%)]\tLoss: 0.094546 0.150529\n",
      "Train Epoch: 0 [1420/5636 (25%)]\tLoss: 0.083009 0.160083\n",
      "Train Epoch: 0 [1440/5636 (26%)]\tLoss: 0.095254 0.191311\n",
      "Train Epoch: 0 [1460/5636 (26%)]\tLoss: 0.094035 0.145074\n",
      "Train Epoch: 0 [1480/5636 (26%)]\tLoss: 0.088830 0.179381\n",
      "Train Epoch: 0 [1500/5636 (27%)]\tLoss: 0.100415 0.175044\n",
      "Train Epoch: 0 [1520/5636 (27%)]\tLoss: 0.094962 0.144745\n",
      "Train Epoch: 0 [1540/5636 (27%)]\tLoss: 0.079920 0.146821\n",
      "Train Epoch: 0 [1560/5636 (28%)]\tLoss: 0.080951 0.152050\n",
      "Train Epoch: 0 [1580/5636 (28%)]\tLoss: 0.086939 0.167201\n",
      "Train Epoch: 0 [1600/5636 (28%)]\tLoss: 0.094866 0.150028\n",
      "Train Epoch: 0 [1620/5636 (29%)]\tLoss: 0.086079 0.165239\n",
      "Train Epoch: 0 [1640/5636 (29%)]\tLoss: 0.078556 0.130783\n",
      "Train Epoch: 0 [1660/5636 (29%)]\tLoss: 0.089997 0.157310\n",
      "Train Epoch: 0 [1680/5636 (30%)]\tLoss: 0.081752 0.129784\n",
      "Train Epoch: 0 [1700/5636 (30%)]\tLoss: 0.092086 0.182012\n",
      "Train Epoch: 0 [1720/5636 (30%)]\tLoss: 0.092728 0.127016\n",
      "Train Epoch: 0 [1740/5636 (31%)]\tLoss: 0.091741 0.137224\n",
      "Train Epoch: 0 [1760/5636 (31%)]\tLoss: 0.089340 0.176257\n",
      "Train Epoch: 0 [1780/5636 (32%)]\tLoss: 0.087861 0.157039\n",
      "Train Epoch: 0 [1800/5636 (32%)]\tLoss: 0.100055 0.160865\n",
      "Train Epoch: 0 [1820/5636 (32%)]\tLoss: 0.080893 0.158750\n",
      "Train Epoch: 0 [1840/5636 (33%)]\tLoss: 0.094262 0.153615\n",
      "Train Epoch: 0 [1860/5636 (33%)]\tLoss: 0.084762 0.138481\n",
      "Train Epoch: 0 [1880/5636 (33%)]\tLoss: 0.088120 0.161319\n",
      "Train Epoch: 0 [1900/5636 (34%)]\tLoss: 0.065639 0.138577\n",
      "Train Epoch: 0 [1920/5636 (34%)]\tLoss: 0.096332 0.172480\n",
      "Train Epoch: 0 [1940/5636 (34%)]\tLoss: 0.094502 0.148008\n",
      "Train Epoch: 0 [1960/5636 (35%)]\tLoss: 0.084050 0.159884\n",
      "Train Epoch: 0 [1980/5636 (35%)]\tLoss: 0.095412 0.152759\n",
      "Train Epoch: 0 [2000/5636 (35%)]\tLoss: 0.086274 0.180483\n",
      "Train Epoch: 0 [2020/5636 (36%)]\tLoss: 0.085109 0.144165\n",
      "Train Epoch: 0 [2040/5636 (36%)]\tLoss: 0.092577 0.171682\n",
      "Train Epoch: 0 [2060/5636 (37%)]\tLoss: 0.088675 0.164181\n",
      "Train Epoch: 0 [2080/5636 (37%)]\tLoss: 0.074413 0.153488\n",
      "Train Epoch: 0 [2100/5636 (37%)]\tLoss: 0.098847 0.175878\n",
      "Train Epoch: 0 [2120/5636 (38%)]\tLoss: 0.087501 0.147022\n",
      "Train Epoch: 0 [2140/5636 (38%)]\tLoss: 0.074839 0.158077\n",
      "Train Epoch: 0 [2160/5636 (38%)]\tLoss: 0.079304 0.131319\n",
      "Train Epoch: 0 [2180/5636 (39%)]\tLoss: 0.074473 0.151147\n",
      "Train Epoch: 0 [2200/5636 (39%)]\tLoss: 0.082478 0.149536\n",
      "Train Epoch: 0 [2220/5636 (39%)]\tLoss: 0.084845 0.156278\n",
      "Train Epoch: 0 [2240/5636 (40%)]\tLoss: 0.088753 0.135909\n",
      "Train Epoch: 0 [2260/5636 (40%)]\tLoss: 0.076237 0.149412\n",
      "Train Epoch: 0 [2280/5636 (40%)]\tLoss: 0.085422 0.162924\n",
      "Train Epoch: 0 [2300/5636 (41%)]\tLoss: 0.081246 0.148479\n",
      "Train Epoch: 0 [2320/5636 (41%)]\tLoss: 0.081324 0.140845\n",
      "Train Epoch: 0 [2340/5636 (41%)]\tLoss: 0.080639 0.146633\n",
      "Train Epoch: 0 [2360/5636 (42%)]\tLoss: 0.086997 0.176559\n",
      "Train Epoch: 0 [2380/5636 (42%)]\tLoss: 0.074967 0.138371\n",
      "Train Epoch: 0 [2400/5636 (43%)]\tLoss: 0.081079 0.149536\n",
      "Train Epoch: 0 [2420/5636 (43%)]\tLoss: 0.088917 0.148324\n",
      "Train Epoch: 0 [2440/5636 (43%)]\tLoss: 0.072529 0.168633\n",
      "Train Epoch: 0 [2460/5636 (44%)]\tLoss: 0.084054 0.165980\n",
      "Train Epoch: 0 [2480/5636 (44%)]\tLoss: 0.082620 0.153099\n",
      "Train Epoch: 0 [2500/5636 (44%)]\tLoss: 0.092768 0.141659\n",
      "Train Epoch: 0 [2520/5636 (45%)]\tLoss: 0.089370 0.171349\n",
      "Train Epoch: 0 [2540/5636 (45%)]\tLoss: 0.090005 0.162842\n",
      "Train Epoch: 0 [2560/5636 (45%)]\tLoss: 0.081812 0.160182\n",
      "Train Epoch: 0 [2580/5636 (46%)]\tLoss: 0.086084 0.168299\n",
      "Train Epoch: 0 [2600/5636 (46%)]\tLoss: 0.082909 0.153155\n",
      "Train Epoch: 0 [2620/5636 (46%)]\tLoss: 0.083165 0.150271\n",
      "Train Epoch: 0 [2640/5636 (47%)]\tLoss: 0.081909 0.180980\n",
      "Train Epoch: 0 [2660/5636 (47%)]\tLoss: 0.090943 0.147068\n",
      "Train Epoch: 0 [2680/5636 (48%)]\tLoss: 0.089272 0.156778\n",
      "Train Epoch: 0 [2700/5636 (48%)]\tLoss: 0.078761 0.156592\n",
      "Train Epoch: 0 [2720/5636 (48%)]\tLoss: 0.080568 0.161951\n",
      "Train Epoch: 0 [2740/5636 (49%)]\tLoss: 0.081545 0.154438\n",
      "Train Epoch: 0 [2760/5636 (49%)]\tLoss: 0.097029 0.167970\n",
      "Train Epoch: 0 [2780/5636 (49%)]\tLoss: 0.076444 0.159328\n",
      "Train Epoch: 0 [2800/5636 (50%)]\tLoss: 0.088116 0.166237\n",
      "Train Epoch: 0 [2820/5636 (50%)]\tLoss: 0.065530 0.133152\n",
      "Train Epoch: 0 [2840/5636 (50%)]\tLoss: 0.071647 0.137105\n",
      "Train Epoch: 0 [2860/5636 (51%)]\tLoss: 0.076980 0.135954\n",
      "Train Epoch: 0 [2880/5636 (51%)]\tLoss: 0.088319 0.159718\n",
      "Train Epoch: 0 [2900/5636 (51%)]\tLoss: 0.082235 0.150473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [2920/5636 (52%)]\tLoss: 0.079906 0.146520\n",
      "Train Epoch: 0 [2940/5636 (52%)]\tLoss: 0.084533 0.157414\n",
      "Train Epoch: 0 [2960/5636 (52%)]\tLoss: 0.076644 0.171996\n",
      "Train Epoch: 0 [2980/5636 (53%)]\tLoss: 0.091239 0.162553\n",
      "Train Epoch: 0 [3000/5636 (53%)]\tLoss: 0.091698 0.167863\n",
      "Train Epoch: 0 [3020/5636 (54%)]\tLoss: 0.088266 0.168816\n",
      "Train Epoch: 0 [3040/5636 (54%)]\tLoss: 0.093399 0.171812\n",
      "Train Epoch: 0 [3060/5636 (54%)]\tLoss: 0.084839 0.166303\n",
      "Train Epoch: 0 [3080/5636 (55%)]\tLoss: 0.075990 0.151718\n",
      "Train Epoch: 0 [3100/5636 (55%)]\tLoss: 0.092883 0.151905\n",
      "Train Epoch: 0 [3120/5636 (55%)]\tLoss: 0.081938 0.154922\n",
      "Train Epoch: 0 [3140/5636 (56%)]\tLoss: 0.078955 0.167775\n",
      "Train Epoch: 0 [3160/5636 (56%)]\tLoss: 0.087610 0.157739\n",
      "Train Epoch: 0 [3180/5636 (56%)]\tLoss: 0.082801 0.149474\n",
      "Train Epoch: 0 [3200/5636 (57%)]\tLoss: 0.070755 0.155491\n",
      "Train Epoch: 0 [3220/5636 (57%)]\tLoss: 0.087373 0.151400\n",
      "Train Epoch: 0 [3240/5636 (57%)]\tLoss: 0.081960 0.162344\n",
      "Train Epoch: 0 [3260/5636 (58%)]\tLoss: 0.077312 0.152868\n",
      "Train Epoch: 0 [3280/5636 (58%)]\tLoss: 0.087015 0.165172\n",
      "Train Epoch: 0 [3300/5636 (59%)]\tLoss: 0.086088 0.139596\n",
      "Train Epoch: 0 [3320/5636 (59%)]\tLoss: 0.080639 0.170666\n",
      "Train Epoch: 0 [3340/5636 (59%)]\tLoss: 0.087445 0.155367\n",
      "Train Epoch: 0 [3360/5636 (60%)]\tLoss: 0.085724 0.160011\n",
      "Train Epoch: 0 [3380/5636 (60%)]\tLoss: 0.080498 0.163258\n",
      "Train Epoch: 0 [3400/5636 (60%)]\tLoss: 0.080396 0.163049\n",
      "Train Epoch: 0 [3420/5636 (61%)]\tLoss: 0.076970 0.146062\n",
      "Train Epoch: 0 [3440/5636 (61%)]\tLoss: 0.087732 0.170073\n",
      "Train Epoch: 0 [3460/5636 (61%)]\tLoss: 0.086042 0.150210\n",
      "Train Epoch: 0 [3480/5636 (62%)]\tLoss: 0.083828 0.156664\n",
      "Train Epoch: 0 [3500/5636 (62%)]\tLoss: 0.082276 0.177833\n",
      "Train Epoch: 0 [3520/5636 (62%)]\tLoss: 0.086820 0.142103\n",
      "Train Epoch: 0 [3540/5636 (63%)]\tLoss: 0.069440 0.139206\n",
      "Train Epoch: 0 [3560/5636 (63%)]\tLoss: 0.074347 0.150410\n",
      "Train Epoch: 0 [3580/5636 (63%)]\tLoss: 0.084168 0.162859\n",
      "Train Epoch: 0 [3600/5636 (64%)]\tLoss: 0.082119 0.149537\n",
      "Train Epoch: 0 [3620/5636 (64%)]\tLoss: 0.080079 0.166061\n",
      "Train Epoch: 0 [3640/5636 (65%)]\tLoss: 0.082555 0.154085\n",
      "Train Epoch: 0 [3660/5636 (65%)]\tLoss: 0.083748 0.156796\n",
      "Train Epoch: 0 [3680/5636 (65%)]\tLoss: 0.086721 0.161463\n",
      "Train Epoch: 0 [3700/5636 (66%)]\tLoss: 0.076168 0.157188\n",
      "Train Epoch: 0 [3720/5636 (66%)]\tLoss: 0.090834 0.166182\n",
      "Train Epoch: 0 [3740/5636 (66%)]\tLoss: 0.086820 0.166823\n",
      "Train Epoch: 0 [3760/5636 (67%)]\tLoss: 0.093381 0.171473\n",
      "Train Epoch: 0 [3780/5636 (67%)]\tLoss: 0.087538 0.156517\n",
      "Train Epoch: 0 [3800/5636 (67%)]\tLoss: 0.091530 0.165542\n",
      "Train Epoch: 0 [3820/5636 (68%)]\tLoss: 0.076332 0.166099\n",
      "Train Epoch: 0 [3840/5636 (68%)]\tLoss: 0.086219 0.163984\n",
      "Train Epoch: 0 [3860/5636 (68%)]\tLoss: 0.080462 0.148745\n",
      "Train Epoch: 0 [3880/5636 (69%)]\tLoss: 0.077142 0.162991\n",
      "Train Epoch: 0 [3900/5636 (69%)]\tLoss: 0.082018 0.160607\n",
      "Train Epoch: 0 [3920/5636 (70%)]\tLoss: 0.084178 0.157429\n",
      "Train Epoch: 0 [3940/5636 (70%)]\tLoss: 0.082212 0.150495\n",
      "Train Epoch: 0 [3960/5636 (70%)]\tLoss: 0.081021 0.158122\n",
      "Train Epoch: 0 [3980/5636 (71%)]\tLoss: 0.081382 0.159401\n",
      "Train Epoch: 0 [4000/5636 (71%)]\tLoss: 0.090584 0.159583\n",
      "Train Epoch: 0 [4020/5636 (71%)]\tLoss: 0.082083 0.147811\n",
      "Train Epoch: 0 [4040/5636 (72%)]\tLoss: 0.081573 0.158837\n",
      "Train Epoch: 0 [4060/5636 (72%)]\tLoss: 0.082052 0.155821\n",
      "Train Epoch: 0 [4080/5636 (72%)]\tLoss: 0.066580 0.141541\n",
      "Train Epoch: 0 [4100/5636 (73%)]\tLoss: 0.075445 0.148748\n",
      "Train Epoch: 0 [4120/5636 (73%)]\tLoss: 0.081851 0.152561\n",
      "Train Epoch: 0 [4140/5636 (73%)]\tLoss: 0.085613 0.153990\n",
      "Train Epoch: 0 [4160/5636 (74%)]\tLoss: 0.075143 0.143279\n",
      "Train Epoch: 0 [4180/5636 (74%)]\tLoss: 0.079863 0.179322\n",
      "Train Epoch: 0 [4200/5636 (74%)]\tLoss: 0.084136 0.150506\n",
      "Train Epoch: 0 [4220/5636 (75%)]\tLoss: 0.089944 0.162070\n",
      "Train Epoch: 0 [4240/5636 (75%)]\tLoss: 0.076368 0.136413\n",
      "Train Epoch: 0 [4260/5636 (76%)]\tLoss: 0.082811 0.165151\n",
      "Train Epoch: 0 [4280/5636 (76%)]\tLoss: 0.072103 0.143864\n",
      "Train Epoch: 0 [4300/5636 (76%)]\tLoss: 0.087076 0.155404\n",
      "Train Epoch: 0 [4320/5636 (77%)]\tLoss: 0.073062 0.147258\n",
      "Train Epoch: 0 [4340/5636 (77%)]\tLoss: 0.070658 0.135536\n",
      "Train Epoch: 0 [4360/5636 (77%)]\tLoss: 0.077154 0.149187\n",
      "Train Epoch: 0 [4380/5636 (78%)]\tLoss: 0.077756 0.146184\n",
      "Train Epoch: 0 [4400/5636 (78%)]\tLoss: 0.085400 0.157120\n",
      "Train Epoch: 0 [4420/5636 (78%)]\tLoss: 0.075800 0.150725\n",
      "Train Epoch: 0 [4440/5636 (79%)]\tLoss: 0.094058 0.167510\n",
      "Train Epoch: 0 [4460/5636 (79%)]\tLoss: 0.084057 0.159296\n",
      "Train Epoch: 0 [4480/5636 (79%)]\tLoss: 0.088484 0.168548\n",
      "Train Epoch: 0 [4500/5636 (80%)]\tLoss: 0.079689 0.152051\n",
      "Train Epoch: 0 [4520/5636 (80%)]\tLoss: 0.076042 0.143910\n",
      "Train Epoch: 0 [4540/5636 (80%)]\tLoss: 0.085922 0.171272\n",
      "Train Epoch: 0 [4560/5636 (81%)]\tLoss: 0.071847 0.149777\n",
      "Train Epoch: 0 [4580/5636 (81%)]\tLoss: 0.071227 0.132054\n",
      "Train Epoch: 0 [4600/5636 (82%)]\tLoss: 0.079688 0.147820\n",
      "Train Epoch: 0 [4620/5636 (82%)]\tLoss: 0.086366 0.158994\n",
      "Train Epoch: 0 [4640/5636 (82%)]\tLoss: 0.086023 0.162461\n",
      "Train Epoch: 0 [4660/5636 (83%)]\tLoss: 0.079811 0.138893\n",
      "Train Epoch: 0 [4680/5636 (83%)]\tLoss: 0.076765 0.159826\n",
      "Train Epoch: 0 [4700/5636 (83%)]\tLoss: 0.080567 0.151895\n",
      "Train Epoch: 0 [4720/5636 (84%)]\tLoss: 0.081940 0.146026\n",
      "Train Epoch: 0 [4740/5636 (84%)]\tLoss: 0.085906 0.161580\n",
      "Train Epoch: 0 [4760/5636 (84%)]\tLoss: 0.083496 0.156839\n",
      "Train Epoch: 0 [4780/5636 (85%)]\tLoss: 0.085345 0.169750\n",
      "Train Epoch: 0 [4800/5636 (85%)]\tLoss: 0.087689 0.180430\n",
      "Train Epoch: 0 [4820/5636 (85%)]\tLoss: 0.083827 0.162185\n",
      "Train Epoch: 0 [4840/5636 (86%)]\tLoss: 0.081264 0.151184\n",
      "Train Epoch: 0 [4860/5636 (86%)]\tLoss: 0.072919 0.156090\n",
      "Train Epoch: 0 [4880/5636 (87%)]\tLoss: 0.073932 0.146794\n",
      "Train Epoch: 0 [4900/5636 (87%)]\tLoss: 0.082332 0.150693\n",
      "Train Epoch: 0 [4920/5636 (87%)]\tLoss: 0.083842 0.152436\n",
      "Train Epoch: 0 [4940/5636 (88%)]\tLoss: 0.079618 0.147223\n",
      "Train Epoch: 0 [4960/5636 (88%)]\tLoss: 0.079778 0.148625\n",
      "Train Epoch: 0 [4980/5636 (88%)]\tLoss: 0.072036 0.143513\n",
      "Train Epoch: 0 [5000/5636 (89%)]\tLoss: 0.076451 0.159441\n",
      "Train Epoch: 0 [5020/5636 (89%)]\tLoss: 0.081334 0.152769\n",
      "Train Epoch: 0 [5040/5636 (89%)]\tLoss: 0.086196 0.168971\n",
      "Train Epoch: 0 [5060/5636 (90%)]\tLoss: 0.074872 0.150765\n",
      "Train Epoch: 0 [5080/5636 (90%)]\tLoss: 0.078518 0.149556\n",
      "Train Epoch: 0 [5100/5636 (90%)]\tLoss: 0.083651 0.173265\n",
      "Train Epoch: 0 [5120/5636 (91%)]\tLoss: 0.078462 0.145415\n",
      "Train Epoch: 0 [5140/5636 (91%)]\tLoss: 0.078996 0.146059\n",
      "Train Epoch: 0 [5160/5636 (91%)]\tLoss: 0.078438 0.150790\n",
      "Train Epoch: 0 [5180/5636 (92%)]\tLoss: 0.078946 0.150858\n",
      "Train Epoch: 0 [5200/5636 (92%)]\tLoss: 0.076469 0.144478\n",
      "Train Epoch: 0 [5220/5636 (93%)]\tLoss: 0.083344 0.147524\n",
      "Train Epoch: 0 [5240/5636 (93%)]\tLoss: 0.074165 0.138687\n",
      "Train Epoch: 0 [5260/5636 (93%)]\tLoss: 0.090024 0.151990\n",
      "Train Epoch: 0 [5280/5636 (94%)]\tLoss: 0.087740 0.156243\n",
      "Train Epoch: 0 [5300/5636 (94%)]\tLoss: 0.070561 0.154231\n",
      "Train Epoch: 0 [5320/5636 (94%)]\tLoss: 0.076236 0.151696\n",
      "Train Epoch: 0 [5340/5636 (95%)]\tLoss: 0.080183 0.157250\n",
      "Train Epoch: 0 [5360/5636 (95%)]\tLoss: 0.091386 0.153291\n",
      "Train Epoch: 0 [5380/5636 (95%)]\tLoss: 0.083595 0.146652\n",
      "Train Epoch: 0 [5400/5636 (96%)]\tLoss: 0.077428 0.173570\n",
      "Train Epoch: 0 [5420/5636 (96%)]\tLoss: 0.080673 0.144908\n",
      "Train Epoch: 0 [5440/5636 (96%)]\tLoss: 0.082239 0.159055\n",
      "Train Epoch: 0 [5460/5636 (97%)]\tLoss: 0.072781 0.156111\n",
      "Train Epoch: 0 [5480/5636 (97%)]\tLoss: 0.081599 0.155517\n",
      "Train Epoch: 0 [5500/5636 (98%)]\tLoss: 0.068832 0.145867\n",
      "Train Epoch: 0 [5520/5636 (98%)]\tLoss: 0.069896 0.137005\n",
      "Train Epoch: 0 [5540/5636 (98%)]\tLoss: 0.084454 0.147074\n",
      "Train Epoch: 0 [5560/5636 (99%)]\tLoss: 0.076985 0.154773\n",
      "Train Epoch: 0 [5580/5636 (99%)]\tLoss: 0.081028 0.156674\n",
      "Train Epoch: 0 [5600/5636 (99%)]\tLoss: 0.086013 0.138350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([16, 28])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([16, 28])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_5500\\16305181.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [4496/5636 (100%)]\tLoss: 0.087078 0.152378\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0724 \n",
      "Average abs_loss: 0.1501 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n",
      "Train Epoch: 1 [0/5636 (0%)]\tLoss: 0.084011 0.158156\n",
      "Train Epoch: 1 [20/5636 (0%)]\tLoss: 0.084573 0.171233\n",
      "Train Epoch: 1 [40/5636 (1%)]\tLoss: 0.074993 0.150841\n",
      "Train Epoch: 1 [60/5636 (1%)]\tLoss: 0.081469 0.152863\n",
      "Train Epoch: 1 [80/5636 (1%)]\tLoss: 0.085177 0.156848\n",
      "Train Epoch: 1 [100/5636 (2%)]\tLoss: 0.076585 0.149394\n",
      "Train Epoch: 1 [120/5636 (2%)]\tLoss: 0.081360 0.159756\n",
      "Train Epoch: 1 [140/5636 (2%)]\tLoss: 0.074392 0.161171\n",
      "Train Epoch: 1 [160/5636 (3%)]\tLoss: 0.071129 0.136549\n",
      "Train Epoch: 1 [180/5636 (3%)]\tLoss: 0.080771 0.152709\n",
      "Train Epoch: 1 [200/5636 (4%)]\tLoss: 0.089313 0.164808\n",
      "Train Epoch: 1 [220/5636 (4%)]\tLoss: 0.070834 0.142114\n",
      "Train Epoch: 1 [240/5636 (4%)]\tLoss: 0.075146 0.162128\n",
      "Train Epoch: 1 [260/5636 (5%)]\tLoss: 0.078322 0.150204\n",
      "Train Epoch: 1 [280/5636 (5%)]\tLoss: 0.083756 0.163498\n",
      "Train Epoch: 1 [300/5636 (5%)]\tLoss: 0.079025 0.156570\n",
      "Train Epoch: 1 [320/5636 (6%)]\tLoss: 0.084963 0.154274\n",
      "Train Epoch: 1 [340/5636 (6%)]\tLoss: 0.073162 0.139043\n",
      "Train Epoch: 1 [360/5636 (6%)]\tLoss: 0.074608 0.142890\n",
      "Train Epoch: 1 [380/5636 (7%)]\tLoss: 0.078031 0.148733\n",
      "Train Epoch: 1 [400/5636 (7%)]\tLoss: 0.083187 0.159973\n",
      "Train Epoch: 1 [420/5636 (7%)]\tLoss: 0.087023 0.165211\n",
      "Train Epoch: 1 [440/5636 (8%)]\tLoss: 0.087483 0.158710\n",
      "Train Epoch: 1 [460/5636 (8%)]\tLoss: 0.082595 0.146891\n",
      "Train Epoch: 1 [480/5636 (9%)]\tLoss: 0.076318 0.159831\n",
      "Train Epoch: 1 [500/5636 (9%)]\tLoss: 0.078138 0.157123\n",
      "Train Epoch: 1 [520/5636 (9%)]\tLoss: 0.076990 0.145411\n",
      "Train Epoch: 1 [540/5636 (10%)]\tLoss: 0.076389 0.151837\n",
      "Train Epoch: 1 [560/5636 (10%)]\tLoss: 0.083884 0.163203\n",
      "Train Epoch: 1 [580/5636 (10%)]\tLoss: 0.087018 0.151125\n",
      "Train Epoch: 1 [600/5636 (11%)]\tLoss: 0.076409 0.151639\n",
      "Train Epoch: 1 [620/5636 (11%)]\tLoss: 0.072960 0.152601\n",
      "Train Epoch: 1 [640/5636 (11%)]\tLoss: 0.081575 0.148972\n",
      "Train Epoch: 1 [660/5636 (12%)]\tLoss: 0.074500 0.151217\n",
      "Train Epoch: 1 [680/5636 (12%)]\tLoss: 0.077672 0.150333\n",
      "Train Epoch: 1 [700/5636 (12%)]\tLoss: 0.079360 0.140651\n",
      "Train Epoch: 1 [720/5636 (13%)]\tLoss: 0.083988 0.160566\n",
      "Train Epoch: 1 [740/5636 (13%)]\tLoss: 0.079285 0.149852\n",
      "Train Epoch: 1 [760/5636 (13%)]\tLoss: 0.073290 0.142708\n",
      "Train Epoch: 1 [780/5636 (14%)]\tLoss: 0.064465 0.142825\n",
      "Train Epoch: 1 [800/5636 (14%)]\tLoss: 0.078657 0.137974\n",
      "Train Epoch: 1 [820/5636 (15%)]\tLoss: 0.069443 0.130212\n",
      "Train Epoch: 1 [840/5636 (15%)]\tLoss: 0.079912 0.148828\n",
      "Train Epoch: 1 [860/5636 (15%)]\tLoss: 0.083521 0.148114\n",
      "Train Epoch: 1 [880/5636 (16%)]\tLoss: 0.083647 0.161099\n",
      "Train Epoch: 1 [900/5636 (16%)]\tLoss: 0.083623 0.158268\n",
      "Train Epoch: 1 [920/5636 (16%)]\tLoss: 0.088725 0.162096\n",
      "Train Epoch: 1 [940/5636 (17%)]\tLoss: 0.084274 0.166873\n",
      "Train Epoch: 1 [960/5636 (17%)]\tLoss: 0.085741 0.156749\n",
      "Train Epoch: 1 [980/5636 (17%)]\tLoss: 0.079240 0.150599\n",
      "Train Epoch: 1 [1000/5636 (18%)]\tLoss: 0.083981 0.176486\n",
      "Train Epoch: 1 [1020/5636 (18%)]\tLoss: 0.082030 0.158465\n",
      "Train Epoch: 1 [1040/5636 (18%)]\tLoss: 0.088024 0.160611\n",
      "Train Epoch: 1 [1060/5636 (19%)]\tLoss: 0.072719 0.141559\n",
      "Train Epoch: 1 [1080/5636 (19%)]\tLoss: 0.078003 0.144320\n",
      "Train Epoch: 1 [1100/5636 (20%)]\tLoss: 0.085768 0.158751\n",
      "Train Epoch: 1 [1120/5636 (20%)]\tLoss: 0.074208 0.152873\n",
      "Train Epoch: 1 [1140/5636 (20%)]\tLoss: 0.083170 0.166382\n",
      "Train Epoch: 1 [1160/5636 (21%)]\tLoss: 0.064839 0.125978\n",
      "Train Epoch: 1 [1180/5636 (21%)]\tLoss: 0.081836 0.152475\n",
      "Train Epoch: 1 [1200/5636 (21%)]\tLoss: 0.080550 0.158082\n",
      "Train Epoch: 1 [1220/5636 (22%)]\tLoss: 0.081048 0.136652\n",
      "Train Epoch: 1 [1240/5636 (22%)]\tLoss: 0.073426 0.156115\n",
      "Train Epoch: 1 [1260/5636 (22%)]\tLoss: 0.081078 0.151986\n",
      "Train Epoch: 1 [1280/5636 (23%)]\tLoss: 0.083762 0.161220\n",
      "Train Epoch: 1 [1300/5636 (23%)]\tLoss: 0.084805 0.154788\n",
      "Train Epoch: 1 [1320/5636 (23%)]\tLoss: 0.079579 0.157170\n",
      "Train Epoch: 1 [1340/5636 (24%)]\tLoss: 0.081170 0.146698\n",
      "Train Epoch: 1 [1360/5636 (24%)]\tLoss: 0.078879 0.167370\n",
      "Train Epoch: 1 [1380/5636 (24%)]\tLoss: 0.082265 0.148062\n",
      "Train Epoch: 1 [1400/5636 (25%)]\tLoss: 0.079424 0.154782\n",
      "Train Epoch: 1 [1420/5636 (25%)]\tLoss: 0.078164 0.156983\n",
      "Train Epoch: 1 [1440/5636 (26%)]\tLoss: 0.087904 0.167910\n",
      "Train Epoch: 1 [1460/5636 (26%)]\tLoss: 0.083702 0.151325\n",
      "Train Epoch: 1 [1480/5636 (26%)]\tLoss: 0.076460 0.141855\n",
      "Train Epoch: 1 [1500/5636 (27%)]\tLoss: 0.086560 0.169739\n",
      "Train Epoch: 1 [1520/5636 (27%)]\tLoss: 0.081549 0.162152\n",
      "Train Epoch: 1 [1540/5636 (27%)]\tLoss: 0.072218 0.151818\n",
      "Train Epoch: 1 [1560/5636 (28%)]\tLoss: 0.085282 0.157510\n",
      "Train Epoch: 1 [1580/5636 (28%)]\tLoss: 0.074026 0.151396\n",
      "Train Epoch: 1 [1600/5636 (28%)]\tLoss: 0.081220 0.147816\n",
      "Train Epoch: 1 [1620/5636 (29%)]\tLoss: 0.083895 0.161716\n",
      "Train Epoch: 1 [1640/5636 (29%)]\tLoss: 0.078837 0.157608\n",
      "Train Epoch: 1 [1660/5636 (29%)]\tLoss: 0.075762 0.137747\n",
      "Train Epoch: 1 [1680/5636 (30%)]\tLoss: 0.080092 0.148470\n",
      "Train Epoch: 1 [1700/5636 (30%)]\tLoss: 0.076330 0.133911\n",
      "Train Epoch: 1 [1720/5636 (30%)]\tLoss: 0.070288 0.144652\n",
      "Train Epoch: 1 [1740/5636 (31%)]\tLoss: 0.080469 0.159667\n",
      "Train Epoch: 1 [1760/5636 (31%)]\tLoss: 0.087849 0.156795\n",
      "Train Epoch: 1 [1780/5636 (32%)]\tLoss: 0.076459 0.167436\n",
      "Train Epoch: 1 [1800/5636 (32%)]\tLoss: 0.082954 0.145359\n",
      "Train Epoch: 1 [1820/5636 (32%)]\tLoss: 0.074047 0.142751\n",
      "Train Epoch: 1 [1840/5636 (33%)]\tLoss: 0.080290 0.152931\n",
      "Train Epoch: 1 [1860/5636 (33%)]\tLoss: 0.077138 0.151304\n",
      "Train Epoch: 1 [1880/5636 (33%)]\tLoss: 0.086450 0.169648\n",
      "Train Epoch: 1 [1900/5636 (34%)]\tLoss: 0.090303 0.163062\n",
      "Train Epoch: 1 [1920/5636 (34%)]\tLoss: 0.079782 0.150377\n",
      "Train Epoch: 1 [1940/5636 (34%)]\tLoss: 0.065277 0.153648\n",
      "Train Epoch: 1 [1960/5636 (35%)]\tLoss: 0.072459 0.146442\n",
      "Train Epoch: 1 [1980/5636 (35%)]\tLoss: 0.080114 0.145871\n",
      "Train Epoch: 1 [2000/5636 (35%)]\tLoss: 0.069998 0.133395\n",
      "Train Epoch: 1 [2020/5636 (36%)]\tLoss: 0.080904 0.151096\n",
      "Train Epoch: 1 [2040/5636 (36%)]\tLoss: 0.078321 0.147177\n",
      "Train Epoch: 1 [2060/5636 (37%)]\tLoss: 0.077688 0.139846\n",
      "Train Epoch: 1 [2080/5636 (37%)]\tLoss: 0.074495 0.152770\n",
      "Train Epoch: 1 [2100/5636 (37%)]\tLoss: 0.088786 0.168904\n",
      "Train Epoch: 1 [2120/5636 (38%)]\tLoss: 0.082468 0.159221\n",
      "Train Epoch: 1 [2140/5636 (38%)]\tLoss: 0.080690 0.156273\n",
      "Train Epoch: 1 [2160/5636 (38%)]\tLoss: 0.079575 0.161600\n",
      "Train Epoch: 1 [2180/5636 (39%)]\tLoss: 0.082424 0.152766\n",
      "Train Epoch: 1 [2200/5636 (39%)]\tLoss: 0.082240 0.155110\n",
      "Train Epoch: 1 [2220/5636 (39%)]\tLoss: 0.079266 0.151320\n",
      "Train Epoch: 1 [2240/5636 (40%)]\tLoss: 0.072881 0.140098\n",
      "Train Epoch: 1 [2260/5636 (40%)]\tLoss: 0.075404 0.149293\n",
      "Train Epoch: 1 [2280/5636 (40%)]\tLoss: 0.082218 0.161182\n",
      "Train Epoch: 1 [2300/5636 (41%)]\tLoss: 0.073656 0.143110\n",
      "Train Epoch: 1 [2320/5636 (41%)]\tLoss: 0.079904 0.140680\n",
      "Train Epoch: 1 [2340/5636 (41%)]\tLoss: 0.078321 0.157005\n",
      "Train Epoch: 1 [2360/5636 (42%)]\tLoss: 0.076965 0.132875\n",
      "Train Epoch: 1 [2380/5636 (42%)]\tLoss: 0.076743 0.154640\n",
      "Train Epoch: 1 [2400/5636 (43%)]\tLoss: 0.069650 0.148741\n",
      "Train Epoch: 1 [2420/5636 (43%)]\tLoss: 0.075981 0.155461\n",
      "Train Epoch: 1 [2440/5636 (43%)]\tLoss: 0.071790 0.138999\n",
      "Train Epoch: 1 [2460/5636 (44%)]\tLoss: 0.081970 0.148203\n",
      "Train Epoch: 1 [2480/5636 (44%)]\tLoss: 0.070572 0.140628\n",
      "Train Epoch: 1 [2500/5636 (44%)]\tLoss: 0.078058 0.144409\n",
      "Train Epoch: 1 [2520/5636 (45%)]\tLoss: 0.081947 0.161323\n",
      "Train Epoch: 1 [2540/5636 (45%)]\tLoss: 0.077329 0.136724\n",
      "Train Epoch: 1 [2560/5636 (45%)]\tLoss: 0.083094 0.163503\n",
      "Train Epoch: 1 [2580/5636 (46%)]\tLoss: 0.080817 0.146358\n",
      "Train Epoch: 1 [2600/5636 (46%)]\tLoss: 0.080298 0.165647\n",
      "Train Epoch: 1 [2620/5636 (46%)]\tLoss: 0.074932 0.148343\n",
      "Train Epoch: 1 [2640/5636 (47%)]\tLoss: 0.090321 0.165816\n",
      "Train Epoch: 1 [2660/5636 (47%)]\tLoss: 0.080238 0.153048\n",
      "Train Epoch: 1 [2680/5636 (48%)]\tLoss: 0.074464 0.147063\n",
      "Train Epoch: 1 [2700/5636 (48%)]\tLoss: 0.081174 0.151879\n",
      "Train Epoch: 1 [2720/5636 (48%)]\tLoss: 0.076334 0.145679\n",
      "Train Epoch: 1 [2740/5636 (49%)]\tLoss: 0.072531 0.135832\n",
      "Train Epoch: 1 [2760/5636 (49%)]\tLoss: 0.073796 0.144765\n",
      "Train Epoch: 1 [2780/5636 (49%)]\tLoss: 0.073324 0.143213\n",
      "Train Epoch: 1 [2800/5636 (50%)]\tLoss: 0.080776 0.151574\n",
      "Train Epoch: 1 [2820/5636 (50%)]\tLoss: 0.083257 0.154608\n",
      "Train Epoch: 1 [2840/5636 (50%)]\tLoss: 0.085474 0.162581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [2860/5636 (51%)]\tLoss: 0.076247 0.159813\n",
      "Train Epoch: 1 [2880/5636 (51%)]\tLoss: 0.082220 0.152068\n",
      "Train Epoch: 1 [2900/5636 (51%)]\tLoss: 0.083603 0.149042\n",
      "Train Epoch: 1 [2920/5636 (52%)]\tLoss: 0.074250 0.157261\n",
      "Train Epoch: 1 [2940/5636 (52%)]\tLoss: 0.074553 0.149893\n",
      "Train Epoch: 1 [2960/5636 (52%)]\tLoss: 0.089447 0.164380\n",
      "Train Epoch: 1 [2980/5636 (53%)]\tLoss: 0.079358 0.160367\n",
      "Train Epoch: 1 [3000/5636 (53%)]\tLoss: 0.079143 0.142999\n",
      "Train Epoch: 1 [3020/5636 (54%)]\tLoss: 0.073208 0.139227\n",
      "Train Epoch: 1 [3040/5636 (54%)]\tLoss: 0.077037 0.154971\n",
      "Train Epoch: 1 [3060/5636 (54%)]\tLoss: 0.080466 0.157532\n",
      "Train Epoch: 1 [3080/5636 (55%)]\tLoss: 0.082408 0.154313\n",
      "Train Epoch: 1 [3100/5636 (55%)]\tLoss: 0.079968 0.158770\n",
      "Train Epoch: 1 [3120/5636 (55%)]\tLoss: 0.077274 0.161193\n",
      "Train Epoch: 1 [3140/5636 (56%)]\tLoss: 0.081871 0.151823\n",
      "Train Epoch: 1 [3160/5636 (56%)]\tLoss: 0.087868 0.145433\n",
      "Train Epoch: 1 [3180/5636 (56%)]\tLoss: 0.085482 0.157987\n",
      "Train Epoch: 1 [3200/5636 (57%)]\tLoss: 0.079636 0.159542\n",
      "Train Epoch: 1 [3220/5636 (57%)]\tLoss: 0.082055 0.156022\n",
      "Train Epoch: 1 [3240/5636 (57%)]\tLoss: 0.071451 0.156521\n",
      "Train Epoch: 1 [3260/5636 (58%)]\tLoss: 0.067243 0.144138\n",
      "Train Epoch: 1 [3280/5636 (58%)]\tLoss: 0.080887 0.145151\n",
      "Train Epoch: 1 [3300/5636 (59%)]\tLoss: 0.082142 0.155358\n",
      "Train Epoch: 1 [3320/5636 (59%)]\tLoss: 0.077421 0.148239\n",
      "Train Epoch: 1 [3340/5636 (59%)]\tLoss: 0.081635 0.155432\n",
      "Train Epoch: 1 [3360/5636 (60%)]\tLoss: 0.077181 0.154925\n",
      "Train Epoch: 1 [3380/5636 (60%)]\tLoss: 0.074982 0.140135\n",
      "Train Epoch: 1 [3400/5636 (60%)]\tLoss: 0.073131 0.149827\n",
      "Train Epoch: 1 [3420/5636 (61%)]\tLoss: 0.071382 0.137224\n",
      "Train Epoch: 1 [3440/5636 (61%)]\tLoss: 0.068691 0.134357\n",
      "Train Epoch: 1 [3460/5636 (61%)]\tLoss: 0.076113 0.155583\n",
      "Train Epoch: 1 [3480/5636 (62%)]\tLoss: 0.080804 0.146784\n",
      "Train Epoch: 1 [3500/5636 (62%)]\tLoss: 0.080668 0.150121\n",
      "Train Epoch: 1 [3520/5636 (62%)]\tLoss: 0.074511 0.147398\n",
      "Train Epoch: 1 [3540/5636 (63%)]\tLoss: 0.090056 0.171393\n",
      "Train Epoch: 1 [3560/5636 (63%)]\tLoss: 0.077928 0.149797\n",
      "Train Epoch: 1 [3580/5636 (63%)]\tLoss: 0.078904 0.160455\n",
      "Train Epoch: 1 [3600/5636 (64%)]\tLoss: 0.069051 0.141069\n",
      "Train Epoch: 1 [3620/5636 (64%)]\tLoss: 0.081044 0.151140\n",
      "Train Epoch: 1 [3640/5636 (65%)]\tLoss: 0.074966 0.137285\n",
      "Train Epoch: 1 [3660/5636 (65%)]\tLoss: 0.073144 0.144878\n",
      "Train Epoch: 1 [3680/5636 (65%)]\tLoss: 0.083866 0.159143\n",
      "Train Epoch: 1 [3700/5636 (66%)]\tLoss: 0.068375 0.136929\n",
      "Train Epoch: 1 [3720/5636 (66%)]\tLoss: 0.076066 0.150391\n",
      "Train Epoch: 1 [3740/5636 (66%)]\tLoss: 0.084159 0.155404\n",
      "Train Epoch: 1 [3760/5636 (67%)]\tLoss: 0.078647 0.157659\n",
      "Train Epoch: 1 [3780/5636 (67%)]\tLoss: 0.074298 0.141366\n",
      "Train Epoch: 1 [3800/5636 (67%)]\tLoss: 0.076174 0.154857\n",
      "Train Epoch: 1 [3820/5636 (68%)]\tLoss: 0.070450 0.136363\n",
      "Train Epoch: 1 [3840/5636 (68%)]\tLoss: 0.075960 0.160083\n",
      "Train Epoch: 1 [3860/5636 (68%)]\tLoss: 0.074315 0.143906\n",
      "Train Epoch: 1 [3880/5636 (69%)]\tLoss: 0.084939 0.158881\n",
      "Train Epoch: 1 [3900/5636 (69%)]\tLoss: 0.072591 0.142422\n",
      "Train Epoch: 1 [3920/5636 (70%)]\tLoss: 0.069490 0.135627\n",
      "Train Epoch: 1 [3940/5636 (70%)]\tLoss: 0.080815 0.154023\n",
      "Train Epoch: 1 [3960/5636 (70%)]\tLoss: 0.074652 0.142114\n",
      "Train Epoch: 1 [3980/5636 (71%)]\tLoss: 0.074530 0.152991\n",
      "Train Epoch: 1 [4000/5636 (71%)]\tLoss: 0.073198 0.140605\n",
      "Train Epoch: 1 [4020/5636 (71%)]\tLoss: 0.083438 0.155515\n",
      "Train Epoch: 1 [4040/5636 (72%)]\tLoss: 0.070184 0.135427\n",
      "Train Epoch: 1 [4060/5636 (72%)]\tLoss: 0.080775 0.147314\n",
      "Train Epoch: 1 [4080/5636 (72%)]\tLoss: 0.078959 0.160550\n",
      "Train Epoch: 1 [4100/5636 (73%)]\tLoss: 0.068097 0.137215\n",
      "Train Epoch: 1 [4120/5636 (73%)]\tLoss: 0.081873 0.156620\n",
      "Train Epoch: 1 [4140/5636 (73%)]\tLoss: 0.075176 0.148045\n",
      "Train Epoch: 1 [4160/5636 (74%)]\tLoss: 0.086859 0.158758\n",
      "Train Epoch: 1 [4180/5636 (74%)]\tLoss: 0.078407 0.160639\n",
      "Train Epoch: 1 [4200/5636 (74%)]\tLoss: 0.077321 0.152141\n",
      "Train Epoch: 1 [4220/5636 (75%)]\tLoss: 0.068024 0.145726\n",
      "Train Epoch: 1 [4240/5636 (75%)]\tLoss: 0.081017 0.139921\n",
      "Train Epoch: 1 [4260/5636 (76%)]\tLoss: 0.083750 0.149416\n",
      "Train Epoch: 1 [4280/5636 (76%)]\tLoss: 0.082678 0.155454\n",
      "Train Epoch: 1 [4300/5636 (76%)]\tLoss: 0.071985 0.143846\n",
      "Train Epoch: 1 [4320/5636 (77%)]\tLoss: 0.079133 0.151757\n",
      "Train Epoch: 1 [4340/5636 (77%)]\tLoss: 0.082331 0.160218\n",
      "Train Epoch: 1 [4360/5636 (77%)]\tLoss: 0.085965 0.174755\n",
      "Train Epoch: 1 [4380/5636 (78%)]\tLoss: 0.077710 0.149470\n",
      "Train Epoch: 1 [4400/5636 (78%)]\tLoss: 0.083418 0.163425\n",
      "Train Epoch: 1 [4420/5636 (78%)]\tLoss: 0.082098 0.156196\n",
      "Train Epoch: 1 [4440/5636 (79%)]\tLoss: 0.083632 0.154437\n",
      "Train Epoch: 1 [4460/5636 (79%)]\tLoss: 0.079419 0.164309\n",
      "Train Epoch: 1 [4480/5636 (79%)]\tLoss: 0.078858 0.155181\n",
      "Train Epoch: 1 [4500/5636 (80%)]\tLoss: 0.071446 0.144292\n",
      "Train Epoch: 1 [4520/5636 (80%)]\tLoss: 0.074424 0.139551\n",
      "Train Epoch: 1 [4540/5636 (80%)]\tLoss: 0.084754 0.165195\n",
      "Train Epoch: 1 [4560/5636 (81%)]\tLoss: 0.075701 0.158687\n",
      "Train Epoch: 1 [4580/5636 (81%)]\tLoss: 0.075924 0.151726\n",
      "Train Epoch: 1 [4600/5636 (82%)]\tLoss: 0.081911 0.148637\n",
      "Train Epoch: 1 [4620/5636 (82%)]\tLoss: 0.067237 0.140015\n",
      "Train Epoch: 1 [4640/5636 (82%)]\tLoss: 0.069727 0.135405\n",
      "Train Epoch: 1 [4660/5636 (83%)]\tLoss: 0.087119 0.154933\n",
      "Train Epoch: 1 [4680/5636 (83%)]\tLoss: 0.079978 0.161628\n",
      "Train Epoch: 1 [4700/5636 (83%)]\tLoss: 0.078898 0.150662\n",
      "Train Epoch: 1 [4720/5636 (84%)]\tLoss: 0.084890 0.161280\n",
      "Train Epoch: 1 [4740/5636 (84%)]\tLoss: 0.069073 0.151714\n",
      "Train Epoch: 1 [4760/5636 (84%)]\tLoss: 0.078818 0.143651\n",
      "Train Epoch: 1 [4780/5636 (85%)]\tLoss: 0.079390 0.151244\n",
      "Train Epoch: 1 [4800/5636 (85%)]\tLoss: 0.080700 0.160349\n",
      "Train Epoch: 1 [4820/5636 (85%)]\tLoss: 0.081582 0.160841\n",
      "Train Epoch: 1 [4840/5636 (86%)]\tLoss: 0.076797 0.150398\n",
      "Train Epoch: 1 [4860/5636 (86%)]\tLoss: 0.083168 0.163699\n",
      "Train Epoch: 1 [4880/5636 (87%)]\tLoss: 0.086582 0.156188\n",
      "Train Epoch: 1 [4900/5636 (87%)]\tLoss: 0.083457 0.162781\n",
      "Train Epoch: 1 [4920/5636 (87%)]\tLoss: 0.080876 0.156805\n",
      "Train Epoch: 1 [4940/5636 (88%)]\tLoss: 0.068787 0.148448\n",
      "Train Epoch: 1 [4960/5636 (88%)]\tLoss: 0.078520 0.152030\n",
      "Train Epoch: 1 [4980/5636 (88%)]\tLoss: 0.077439 0.153577\n",
      "Train Epoch: 1 [5000/5636 (89%)]\tLoss: 0.084928 0.161927\n",
      "Train Epoch: 1 [5020/5636 (89%)]\tLoss: 0.072829 0.154321\n",
      "Train Epoch: 1 [5040/5636 (89%)]\tLoss: 0.076183 0.145700\n",
      "Train Epoch: 1 [5060/5636 (90%)]\tLoss: 0.073950 0.148370\n",
      "Train Epoch: 1 [5080/5636 (90%)]\tLoss: 0.079062 0.147005\n",
      "Train Epoch: 1 [5100/5636 (90%)]\tLoss: 0.083023 0.158604\n",
      "Train Epoch: 1 [5120/5636 (91%)]\tLoss: 0.076152 0.157521\n",
      "Train Epoch: 1 [5140/5636 (91%)]\tLoss: 0.078658 0.150898\n",
      "Train Epoch: 1 [5160/5636 (91%)]\tLoss: 0.082371 0.153074\n",
      "Train Epoch: 1 [5180/5636 (92%)]\tLoss: 0.084738 0.164363\n",
      "Train Epoch: 1 [5200/5636 (92%)]\tLoss: 0.074260 0.160110\n",
      "Train Epoch: 1 [5220/5636 (93%)]\tLoss: 0.081899 0.155011\n",
      "Train Epoch: 1 [5240/5636 (93%)]\tLoss: 0.084597 0.164106\n",
      "Train Epoch: 1 [5260/5636 (93%)]\tLoss: 0.080520 0.155399\n",
      "Train Epoch: 1 [5280/5636 (94%)]\tLoss: 0.071197 0.139002\n",
      "Train Epoch: 1 [5300/5636 (94%)]\tLoss: 0.073016 0.147104\n",
      "Train Epoch: 1 [5320/5636 (94%)]\tLoss: 0.078915 0.151315\n",
      "Train Epoch: 1 [5340/5636 (95%)]\tLoss: 0.080374 0.156811\n",
      "Train Epoch: 1 [5360/5636 (95%)]\tLoss: 0.071743 0.148142\n",
      "Train Epoch: 1 [5380/5636 (95%)]\tLoss: 0.069859 0.151458\n",
      "Train Epoch: 1 [5400/5636 (96%)]\tLoss: 0.082896 0.153034\n",
      "Train Epoch: 1 [5420/5636 (96%)]\tLoss: 0.086510 0.149208\n",
      "Train Epoch: 1 [5440/5636 (96%)]\tLoss: 0.075985 0.156792\n",
      "Train Epoch: 1 [5460/5636 (97%)]\tLoss: 0.077490 0.150819\n",
      "Train Epoch: 1 [5480/5636 (97%)]\tLoss: 0.082035 0.158777\n",
      "Train Epoch: 1 [5500/5636 (98%)]\tLoss: 0.075655 0.156738\n",
      "Train Epoch: 1 [5520/5636 (98%)]\tLoss: 0.066511 0.141760\n",
      "Train Epoch: 1 [5540/5636 (98%)]\tLoss: 0.082784 0.148767\n",
      "Train Epoch: 1 [5560/5636 (99%)]\tLoss: 0.084881 0.155582\n",
      "Train Epoch: 1 [5580/5636 (99%)]\tLoss: 0.077204 0.142989\n",
      "Train Epoch: 1 [5600/5636 (99%)]\tLoss: 0.075816 0.152969\n",
      "Train Epoch: 1 [4496/5636 (100%)]\tLoss: 0.076195 0.154514\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0723 \n",
      "Average abs_loss: 0.1480 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape=(3, img_rows, img_cols)):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=2)\n",
    "        self.conv1_drop = nn.Dropout2d(p=dropout[0])\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout[1])\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv3_drop = nn.Dropout2d(p=dropout[2])\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv4_drop = nn.Dropout2d(p=dropout[3])\n",
    "        self.conv5 = nn.Conv2d(64, 32, kernel_size=2)\n",
    "        self.conv5_drop = nn.Dropout2d(p=dropout[4])\n",
    "        self.conv6 = nn.Conv2d(32, 16, kernel_size=2)\n",
    "        self.conv6_drop = nn.Dropout2d(p=dropout[5])\n",
    "        \n",
    "        n_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_size, 16)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout[6])\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc2_drop = nn.Dropout(p=dropout[7])\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc3_drop = nn.Dropout(p=dropout[8])\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = Variable(torch.rand(bs, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_drop(self.fc2(x)))\n",
    "        x = F.relu(self.fc3_drop(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        human_loss= human_criterion(output, target)\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data, human_loss.data))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    human_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        human_loss += human_criterion(output, target)\n",
    "        if loss==0:\n",
    "            correct+=1\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nAverage abs_loss: {:.4f} \\nGuessed 100% correct: {:.4f}\\n'.format(test_loss.data/i, human_loss.data/i, correct))\n",
    "\n",
    "model.float()\n",
    "print(model)\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    #cross-entropy for classification evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "img_rows, img_cols = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\carol/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-5 torch 1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# the data holders\n",
    "x_test_yolo = []\n",
    "x_train_yolo = []\n",
    "y_test_yolo = []\n",
    "y_train_yolo = []\n",
    "\n",
    "#images need to have the same size!!\n",
    "flist=glob.glob('./Movie_Poster_Dataset_Cropped_Once/*.jpg')\n",
    "\n",
    "#pretrained YOLOv5 model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "length=int(len(flist)*training_size)\n",
    "i = 0\n",
    "\n",
    "#create lists with input data (object confidence vector) and output data (multi-hot encoded genre vectors)\n",
    "for filename in flist:\n",
    "        \n",
    "    imdb_id = filename[filename.index(\"tt\"):filename.index(\".jpg\")]\n",
    "      \n",
    "    if imdb_id in multihot_dict:\n",
    "        img = np.array(cv2.imread(filename))\n",
    "        img = np.swapaxes(img, 2,0)\n",
    "        img = np.swapaxes(img, 2,1)\n",
    "        \n",
    "        results = yolo_model(img, size = 100)\n",
    "            \n",
    "        #create an array for the 91 object categories and set initial confidence to 0\n",
    "        obj_arr = np.empty([91])\n",
    "        for x in range(91):\n",
    "            obj_arr[x] = 0.0\n",
    "\n",
    "        #update the confidence values according to the object detection results\n",
    "        for obj in results.pandas().xyxy[0]:\n",
    "            index =  results.pandas().xyxy[0]['class']\n",
    "            obj_arr[index] = obj_arr[index] + results.pandas().xyxy[0].confidence\n",
    "        \n",
    "        #create multi-hot encoded genre vector\n",
    "        genre_arr = np.empty([28])\n",
    "\n",
    "        for j in range(len(multihot_dict[imdb_id])):\n",
    "            genre_arr[j] = multihot_dict[imdb_id][j]\n",
    "        \n",
    "        if(i<length):                   \n",
    "            x_train_yolo.append(obj_arr)\n",
    "            y_train_yolo.append(genre_arr)\n",
    "        else:\n",
    "            x_test_yolo.append(obj_arr)\n",
    "            y_test_yolo.append(genre_arr)\n",
    "        \n",
    "        i +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5636, 91)\n",
      "5636 train samples\n",
      "2416 test samples\n"
     ]
    }
   ],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train_yolo=np.asarray(x_train_yolo,dtype=float)\n",
    "x_test_yolo=np.asarray(x_test_yolo,dtype=float)\n",
    "y_train_yolo=np.asarray(y_train_yolo,dtype=float)\n",
    "y_test_yolo=np.asarray(y_test_yolo,dtype=float)\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train_yolo.shape)\n",
    "print(x_train_yolo.shape[0], 'train samples')\n",
    "print(x_test_yolo.shape[0], 'test samples')\n",
    "\n",
    "train_length = x_train_yolo.shape[0]\n",
    "\n",
    "x_train_yolo=torch.from_numpy(x_train_yolo)\n",
    "x_test_yolo=torch.from_numpy(x_test_yolo)\n",
    "y_train_yolo=torch.from_numpy(y_train_yolo)\n",
    "y_test_yolo=torch.from_numpy(y_test_yolo)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train_yolo, y_train_yolo)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test_yolo, y_test_yolo)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#fully connected layer after object detection\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(91, 28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = Net()\n",
    "\n",
    "result = model.train()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "genre_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
