{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 8.5 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     ------------------------------------- 307.0/307.0 KB 19.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.8.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 5)) (1.22.3)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 6)) (4.5.5.64)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 7)) (9.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 11)) (1.11.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 12)) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 13)) (4.64.0)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 20)) (1.4.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 21)) (0.11.2)\n",
      "Requirement already satisfied: thop in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 37)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 9)) (3.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.7.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 11)) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.41.0->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 13)) (0.4.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (2.6.6)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.20.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.46.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (58.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.37.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.1.4->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 20)) (2022.1)\n",
      "Requirement already satisfied: six in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\carol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt (line 16)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V0MlomrGAhhg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./Movie_Poster_Metadata/groundtruth\"\n",
    "temp_path = \"./Movie_Poster_Metadata/temp_groundtruth\"\n",
    "path2 = \"./Movie_Poster_Metadata/updated_groundtruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input file and creating a clean one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(path1)\n",
    " \n",
    "if not os.path.exists(temp_path):\n",
    "  os.makedirs(temp_path)    \n",
    "\n",
    "if not os.path.exists(path2):\n",
    "  os.makedirs(path2)\n",
    "\n",
    "\n",
    "\n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(path1+'/'+file_name,'r',encoding='utf-16-le') as file1:\n",
    "\n",
    "        temp_file = open(temp_path+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        for line in file1.readlines():\n",
    "\n",
    "            line = line.replace(\"}\\n\",\"},\\n\")\n",
    "            \n",
    "            # reading all lines that begin with \"  \"_id\"\"\n",
    "            y = re.findall(\"^  \\\"_id\\\"\", line)\n",
    "            if not y:\n",
    "                temp_file.write(line)\n",
    "\n",
    "    file1.close()\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(temp_path)\n",
    " \n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(temp_path+'/'+file_name,'r',encoding='utf-8') as temp_file:\n",
    "    \n",
    "        file2 = open(path2+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        lines = temp_file.readlines()\n",
    "        lines = lines[1:-1]\n",
    "\n",
    "        file2.write(\"[{\")\n",
    "        file2.writelines(lines)\n",
    "        file2.write(\"}]\")\n",
    "        \n",
    "    temp_file.close()\n",
    "    file2.close()\n",
    "\n",
    "shutil.rmtree(temp_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to append all the json objects into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbID        object\n",
      "Director      object\n",
      "Genre         object\n",
      "imdbRating    object\n",
      "dtype: object\n",
      "       imdbID                                  Director  \\\n",
      "0   tt0080684                            Irvin Kershner   \n",
      "1   tt0081562                            Sidney Poitier   \n",
      "2   tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3   tt0080377                            Buddy Van Horn   \n",
      "4   tt0081375                              Howard Zieff   \n",
      "5   tt0080549                             Michael Apted   \n",
      "6   tt0081529                               Hal Needham   \n",
      "7   tt0080453                            Randal Kleiser   \n",
      "8   tt0080455                               John Landis   \n",
      "9   tt0081283                            Robert Redford   \n",
      "10  tt0081353                             Robert Altman   \n",
      "11  tt0081696                             James Bridges   \n",
      "12  tt0081505                           Stanley Kubrick   \n",
      "13  tt0081480                              Jay Sandrich   \n",
      "14  tt0080520                               Tommy Chong   \n",
      "15  tt0080487                              Harold Ramis   \n",
      "16  tt0080474                          Stuart Rosenberg   \n",
      "17  tt0081060                               Ron Maxwell   \n",
      "18  tt0080661                            Brian De Palma   \n",
      "19  tt0080948                         Richard Fleischer   \n",
      "\n",
      "                         Genre imdbRating  \n",
      "0   Action, Adventure, Fantasy        8.8  \n",
      "1                Comedy, Crime        6.8  \n",
      "2                       Comedy        7.8  \n",
      "3               Action, Comedy        6.0  \n",
      "4                  Comedy, War        6.1  \n",
      "5      Biography, Drama, Music        7.5  \n",
      "6               Action, Comedy        5.1  \n",
      "7    Adventure, Drama, Romance        5.7  \n",
      "8        Action, Comedy, Crime        7.9  \n",
      "9                        Drama        7.8  \n",
      "10   Adventure, Comedy, Family        5.2  \n",
      "11     Drama, Romance, Western        6.2  \n",
      "12               Drama, Horror        8.4  \n",
      "13             Comedy, Romance        6.7  \n",
      "14       Comedy, Crime, Sci-Fi        6.1  \n",
      "15               Comedy, Sport        7.4  \n",
      "16                       Drama        7.1  \n",
      "17               Comedy, Drama        6.4  \n",
      "18           Mystery, Thriller        7.1  \n",
      "19       Drama, Music, Romance        5.7  \n",
      "(8873, 4)\n"
     ]
    }
   ],
   "source": [
    "dir_list = os.listdir(path2)\n",
    "\n",
    "movies_df = pd.DataFrame()\n",
    "\n",
    "for file_name in dir_list:    \n",
    "\n",
    "#     try:\n",
    "    df = pd.read_json(path2+'/'+file_name,encoding='utf-8',orient='records')\n",
    "    df = df[['imdbID','Director','Genre','imdbRating']]\n",
    "    movies_df = pd.concat([movies_df,df], ignore_index=True)\n",
    "\n",
    "#     except:\n",
    "#         print(file_name)\n",
    "        \n",
    "print(movies_df.dtypes)\n",
    "print(movies_df.head(20))\n",
    "print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multi-hot encoded genre vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action' 'Comedy' 'Biography' 'Adventure' 'Drama' 'Mystery' 'Fantasy'\n",
      " 'Crime' 'Horror' 'Music' 'Western' 'Documentary' 'Family' 'Animation'\n",
      " 'Romance' 'Musical' 'Sci-Fi' 'Thriller' 'Short' 'War' 'Adult'\n",
      " 'Reality-TV' 'N/A' 'Game-Show' 'History' 'News' 'Sport' 'Talk-Show' None]\n",
      "      imdbID                                  Director  \\\n",
      "0  tt0080684                            Irvin Kershner   \n",
      "1  tt0081562                            Sidney Poitier   \n",
      "2  tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3  tt0080377                            Buddy Van Horn   \n",
      "4  tt0081375                              Howard Zieff   \n",
      "5  tt0080549                             Michael Apted   \n",
      "6  tt0081529                               Hal Needham   \n",
      "7  tt0080453                            Randal Kleiser   \n",
      "8  tt0080455                               John Landis   \n",
      "9  tt0081283                            Robert Redford   \n",
      "\n",
      "                        Genre imdbRating Genre0 Genre1 Genre2 Genre3  \n",
      "0  Action, Adventure, Fantasy        8.8      0      3      6   None  \n",
      "1               Comedy, Crime        6.8      1      7   None   None  \n",
      "2                      Comedy        7.8      1   None   None   None  \n",
      "3              Action, Comedy        6.0      0      1   None   None  \n",
      "4                 Comedy, War        6.1      1     19   None   None  \n",
      "5     Biography, Drama, Music        7.5      2      4      9   None  \n",
      "6              Action, Comedy        5.1      0      1   None   None  \n",
      "7   Adventure, Drama, Romance        5.7      3      4     14   None  \n",
      "8       Action, Comedy, Crime        7.9      0      1      7   None  \n",
      "9                       Drama        7.8      4   None   None   None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_84916\\3553992389.py:9: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n",
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_84916\\3553992389.py:9: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n"
     ]
    }
   ],
   "source": [
    "#split genres in movies_df\n",
    "genres_df = movies_df['Genre'].str.split(', ', expand=True)\n",
    "genres_df.columns = ['Genre'+str(i) for i in genres_df.columns]\n",
    "movies_expanded_df = pd.concat([movies_df,genres_df], axis=1)\n",
    "\n",
    "#print(movies_expanded_df.head(10))\n",
    "\n",
    "#get unique genres\n",
    "unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n",
    "print(unique_genres)\n",
    "#print(np.where(unique_genres == \"Biography\")[0][0])\n",
    "\n",
    "#replace genre strings with ids\n",
    "for genre in unique_genres:\n",
    "    if genre != None:\n",
    "        replacement = np.where(unique_genres == genre)[0][0]\n",
    "        \n",
    "        movies_expanded_df[\"Genre0\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre1\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre2\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre3\"].replace(genre, str(replacement), inplace=True)\n",
    "print(movies_expanded_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      imdbID                                  Director  \\\n",
      "0  tt0080684                            Irvin Kershner   \n",
      "1  tt0081562                            Sidney Poitier   \n",
      "2  tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3  tt0080377                            Buddy Van Horn   \n",
      "4  tt0081375                              Howard Zieff   \n",
      "5  tt0080549                             Michael Apted   \n",
      "6  tt0081529                               Hal Needham   \n",
      "7  tt0080453                            Randal Kleiser   \n",
      "8  tt0080455                               John Landis   \n",
      "9  tt0081283                            Robert Redford   \n",
      "\n",
      "                        Genre imdbRating Genre0 Genre1 Genre2 Genre3  ID_0  \\\n",
      "0  Action, Adventure, Fantasy        8.8      0      3      6   None     1   \n",
      "1               Comedy, Crime        6.8      1      7   None   None     0   \n",
      "2                      Comedy        7.8      1   None   None   None     0   \n",
      "3              Action, Comedy        6.0      0      1   None   None     1   \n",
      "4                 Comedy, War        6.1      1     19   None   None     0   \n",
      "5     Biography, Drama, Music        7.5      2      4      9   None     0   \n",
      "6              Action, Comedy        5.1      0      1   None   None     1   \n",
      "7   Adventure, Drama, Romance        5.7      3      4     14   None     0   \n",
      "8       Action, Comedy, Crime        7.9      0      1      7   None     1   \n",
      "9                       Drama        7.8      4   None   None   None     0   \n",
      "\n",
      "   ID_1  ID_2  ID_3  ID_4  ID_5  ID_6  ID_7  ID_8  ID_9  \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     1     0     0     0     0     0     0     0     0  \n",
      "2     1     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4     1     0     0     0     0     0     0     0     0  \n",
      "5     0     1     0     0     0     0     0     0     0  \n",
      "6     0     0     0     0     0     0     0     0     0  \n",
      "7     0     0     1     0     0     0     0     0     0  \n",
      "8     0     0     0     0     0     0     0     0     0  \n",
      "9     0     0     0     1     0     0     0     0     0  \n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(movies_expanded_df[\"Genre0\"])\n",
    "\n",
    "new_col_names = [\"ID_%s\" % c for c in mlb.classes_]\n",
    "\n",
    "# Create new DataFrame with transformed/one-hot encoded IDs\n",
    "ids = pd.DataFrame(mlb.fit_transform(movies_expanded_df[\"Genre0\"]), columns=new_col_names)\n",
    "\n",
    "# Concat with original \"Genre\" column\n",
    "genres_df = pd.concat( [movies_expanded_df, ids], axis=1 )\n",
    "\n",
    "print(genres_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = \"./Movie_Poster_Dataset\"\n",
    "\n",
    "# Going through all jpg-files, they are chopped up into 100x100 chunks and saved into a new folder\n",
    "for dirname in os.listdir(path3):\n",
    "    for filename in os.listdir(path3 + \"/\" + dirname):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if(ext == '.jpg'):\n",
    "            image = Image.open(os.path.join(path3 + \"/\" + dirname, filename))\n",
    "            width, height = image.size\n",
    "            chopsize = 100\n",
    "            for x0 in range(0, width, chopsize):\n",
    "                for y0 in range(0, height, chopsize):\n",
    "                    if(y0+chopsize <= height and x0+chopsize <= width):\n",
    "                        box = (x0, y0, x0+chopsize, y0+chopsize)\n",
    "                        image.crop(box).save('./Movie_Poster_Dataset_Cropped/%s.x%03d.y%03d.jpg' % (filename.replace('.jpg',''), x0, y0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through a convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "dropout = [0.3, 0.3, 0.3, 0.3, 0.2, 0.2, 0.2, 0.2, 0.15]\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "\n",
    "#the list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('./Movie_Poster_Dataset_Cropped/*.jpg')\n",
    "\n",
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "\n",
    "#currently using 1000 images\n",
    "length=1000*training_size\n",
    "\n",
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in flist:\n",
    "    \n",
    "    imdb_id = filename[0:filename.index(\".\")]\n",
    "            \n",
    "    img = np.array(cv2.imread(filename))\n",
    "    img = np.swapaxes(img, 2,0)\n",
    "    img = np.swapaxes(img, 2,1)\n",
    "\n",
    "    if(i<length):\n",
    "        x_train.append(img)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        #idx = (genres_df.iloc[imdb_id])[Genre0]\n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_train.append(genre_arr)\n",
    "\n",
    "    else:\n",
    "        x_test.append(img)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        #idx = (genres_df.iloc[imdb_id])[Genre0]\n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_test.append(genre_arr)\n",
    "    i+=1\n",
    "    #stopping after 1000 images\n",
    "    if i==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (700, 3, 100, 100)\n",
      "700 train samples\n",
      "300 test samples\n"
     ]
    }
   ],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=np.asarray(x_train,dtype=float)\n",
    "x_test=np.asarray(x_test,dtype=float)\n",
    "y_train=np.asarray(y_train,dtype=float)\n",
    "y_test=np.asarray(y_test,dtype=float)\n",
    "\n",
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv1_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv2): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv5): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv5_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (conv6): Conv2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv6_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3_drop): Dropout(p=0.15, inplace=False)\n",
      "  (fc4): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/700 (0%)]\tLoss: 0.040060 0.074887\n",
      "Train Epoch: 0 [20/700 (3%)]\tLoss: 0.039105 0.068092\n",
      "Train Epoch: 0 [40/700 (6%)]\tLoss: 0.037542 0.057164\n",
      "Train Epoch: 0 [60/700 (9%)]\tLoss: 0.038051 0.075392\n",
      "Train Epoch: 0 [80/700 (11%)]\tLoss: 0.036772 0.049336\n",
      "Train Epoch: 0 [100/700 (14%)]\tLoss: 0.035193 0.054444\n",
      "Train Epoch: 0 [120/700 (17%)]\tLoss: 0.034795 0.065989\n",
      "Train Epoch: 0 [140/700 (20%)]\tLoss: 0.035384 0.065460\n",
      "Train Epoch: 0 [160/700 (23%)]\tLoss: 0.035779 0.057114\n",
      "Train Epoch: 0 [180/700 (26%)]\tLoss: 0.035289 0.058904\n",
      "Train Epoch: 0 [200/700 (29%)]\tLoss: 0.035361 0.058584\n",
      "Train Epoch: 0 [220/700 (31%)]\tLoss: 0.035739 0.063090\n",
      "Train Epoch: 0 [240/700 (34%)]\tLoss: 0.035427 0.057253\n",
      "Train Epoch: 0 [260/700 (37%)]\tLoss: 0.035181 0.063147\n",
      "Train Epoch: 0 [280/700 (40%)]\tLoss: 0.035759 0.064822\n",
      "Train Epoch: 0 [300/700 (43%)]\tLoss: 0.036004 0.059184\n",
      "Train Epoch: 0 [320/700 (46%)]\tLoss: 0.034960 0.059980\n",
      "Train Epoch: 0 [340/700 (49%)]\tLoss: 0.035042 0.059134\n",
      "Train Epoch: 0 [360/700 (51%)]\tLoss: 0.035422 0.063183\n",
      "Train Epoch: 0 [380/700 (54%)]\tLoss: 0.035165 0.062907\n",
      "Train Epoch: 0 [400/700 (57%)]\tLoss: 0.035212 0.062266\n",
      "Train Epoch: 0 [420/700 (60%)]\tLoss: 0.034894 0.064456\n",
      "Train Epoch: 0 [440/700 (63%)]\tLoss: 0.034636 0.067013\n",
      "Train Epoch: 0 [460/700 (66%)]\tLoss: 0.035307 0.061053\n",
      "Train Epoch: 0 [480/700 (69%)]\tLoss: 0.035062 0.068112\n",
      "Train Epoch: 0 [500/700 (71%)]\tLoss: 0.035247 0.062497\n",
      "Train Epoch: 0 [520/700 (74%)]\tLoss: 0.034855 0.072634\n",
      "Train Epoch: 0 [540/700 (77%)]\tLoss: 0.035108 0.057879\n",
      "Train Epoch: 0 [560/700 (80%)]\tLoss: 0.034631 0.064715\n",
      "Train Epoch: 0 [580/700 (83%)]\tLoss: 0.035110 0.061581\n",
      "Train Epoch: 0 [600/700 (86%)]\tLoss: 0.034821 0.064617\n",
      "Train Epoch: 0 [620/700 (89%)]\tLoss: 0.034933 0.066258\n",
      "Train Epoch: 0 [640/700 (91%)]\tLoss: 0.034829 0.065831\n",
      "Train Epoch: 0 [660/700 (94%)]\tLoss: 0.035166 0.056629\n",
      "Train Epoch: 0 [680/700 (97%)]\tLoss: 0.034786 0.067360\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_84916\\559374431.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0344 \n",
      "Average abs_loss: 0.0672 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n",
      "Train Epoch: 1 [0/700 (0%)]\tLoss: 0.034965 0.064767\n",
      "Train Epoch: 1 [20/700 (3%)]\tLoss: 0.034955 0.069272\n",
      "Train Epoch: 1 [40/700 (6%)]\tLoss: 0.034777 0.064887\n",
      "Train Epoch: 1 [60/700 (9%)]\tLoss: 0.034708 0.070597\n",
      "Train Epoch: 1 [80/700 (11%)]\tLoss: 0.034495 0.070145\n",
      "Train Epoch: 1 [100/700 (14%)]\tLoss: 0.034734 0.064066\n",
      "Train Epoch: 1 [120/700 (17%)]\tLoss: 0.034731 0.064574\n",
      "Train Epoch: 1 [140/700 (20%)]\tLoss: 0.034830 0.064226\n",
      "Train Epoch: 1 [160/700 (23%)]\tLoss: 0.034589 0.069115\n",
      "Train Epoch: 1 [180/700 (26%)]\tLoss: 0.034868 0.062671\n",
      "Train Epoch: 1 [200/700 (29%)]\tLoss: 0.034700 0.064548\n",
      "Train Epoch: 1 [220/700 (31%)]\tLoss: 0.034725 0.069050\n",
      "Train Epoch: 1 [240/700 (34%)]\tLoss: 0.034626 0.067209\n",
      "Train Epoch: 1 [260/700 (37%)]\tLoss: 0.034780 0.064893\n",
      "Train Epoch: 1 [280/700 (40%)]\tLoss: 0.034563 0.067498\n",
      "Train Epoch: 1 [300/700 (43%)]\tLoss: 0.034598 0.064039\n",
      "Train Epoch: 1 [320/700 (46%)]\tLoss: 0.034653 0.064781\n",
      "Train Epoch: 1 [340/700 (49%)]\tLoss: 0.034784 0.061880\n",
      "Train Epoch: 1 [360/700 (51%)]\tLoss: 0.034651 0.065339\n",
      "Train Epoch: 1 [380/700 (54%)]\tLoss: 0.034749 0.064056\n",
      "Train Epoch: 1 [400/700 (57%)]\tLoss: 0.034904 0.057272\n",
      "Train Epoch: 1 [420/700 (60%)]\tLoss: 0.034460 0.068884\n",
      "Train Epoch: 1 [440/700 (63%)]\tLoss: 0.034606 0.065530\n",
      "Train Epoch: 1 [460/700 (66%)]\tLoss: 0.034467 0.071342\n",
      "Train Epoch: 1 [480/700 (69%)]\tLoss: 0.034540 0.065883\n",
      "Train Epoch: 1 [500/700 (71%)]\tLoss: 0.034694 0.066003\n",
      "Train Epoch: 1 [520/700 (74%)]\tLoss: 0.034683 0.063197\n",
      "Train Epoch: 1 [540/700 (77%)]\tLoss: 0.034646 0.065902\n",
      "Train Epoch: 1 [560/700 (80%)]\tLoss: 0.034640 0.063762\n",
      "Train Epoch: 1 [580/700 (83%)]\tLoss: 0.034639 0.065965\n",
      "Train Epoch: 1 [600/700 (86%)]\tLoss: 0.034575 0.064985\n",
      "Train Epoch: 1 [620/700 (89%)]\tLoss: 0.034529 0.071396\n",
      "Train Epoch: 1 [640/700 (91%)]\tLoss: 0.034554 0.066290\n",
      "Train Epoch: 1 [660/700 (94%)]\tLoss: 0.034529 0.064018\n",
      "Train Epoch: 1 [680/700 (97%)]\tLoss: 0.034613 0.064369\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0344 \n",
      "Average abs_loss: 0.0705 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape=(3, img_rows, img_cols)):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=2)\n",
    "        self.conv1_drop = nn.Dropout2d(p=dropout[0])\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout[1])\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv3_drop = nn.Dropout2d(p=dropout[2])\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv4_drop = nn.Dropout2d(p=dropout[3])\n",
    "        self.conv5 = nn.Conv2d(64, 32, kernel_size=2)\n",
    "        self.conv5_drop = nn.Dropout2d(p=dropout[4])\n",
    "        self.conv6 = nn.Conv2d(32, 16, kernel_size=2)\n",
    "        self.conv6_drop = nn.Dropout2d(p=dropout[5])\n",
    "        \n",
    "        n_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_size, 16)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout[6])\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc2_drop = nn.Dropout(p=dropout[7])\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc3_drop = nn.Dropout(p=dropout[8])\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = Variable(torch.rand(bs, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_drop(self.fc2(x)))\n",
    "        x = F.relu(self.fc3_drop(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        human_loss= human_criterion(output, target)\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data, human_loss.data))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    human_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        human_loss += human_criterion(output, target)\n",
    "        if loss==0:\n",
    "            correct+=1\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nAverage abs_loss: {:.4f} \\nGuessed 100% correct: {:.4f}\\n'.format(test_loss.data/i, human_loss.data/i, correct))\n",
    "\n",
    "model.float()\n",
    "print(model)\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "img_rows, img_cols = 100, 100\n",
    "\n",
    "x_test_obj = []\n",
    "x_train_obj = []\n",
    "y_test_obj= []\n",
    "y_train_obj= []\n",
    "tempY_obj = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\carol/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-5-5 torch 1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "path = \"./Movie_Poster_Dataset_Cropped\"\n",
    "\n",
    "#pretrained YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "counter = 1000\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if(ext == '.jpg'):\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        results = model(image, size = 100)\n",
    "            \n",
    "        #create an array for the 91 object categories and set initial confidence to 0\n",
    "        obj_arr = np.empty([91])\n",
    "        for x in range(91):\n",
    "            obj_arr[x] = 0.0\n",
    "\n",
    "        #update the confidence values according to the object detection results\n",
    "        for obj in results.pandas().xyxy[0]:\n",
    "            index =  results.pandas().xyxy[0]['class']\n",
    "            obj_arr[index] = obj_arr[index] + results.pandas().xyxy[0].confidence\n",
    "        \n",
    "        x_train_obj.append(obj_arr)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_train_obj.append(genre_arr)\n",
    "        \n",
    "        counter -= 1\n",
    "        if counter == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1000, 91)\n",
      "1000 train samples\n"
     ]
    }
   ],
   "source": [
    "x_train_obj = np.asarray(x_train_obj, dtype=float)\n",
    "y_train_obj = np.asarray(y_train_obj, dtype=float)\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train_obj.shape)\n",
    "print(x_train_obj.shape[0], 'train samples')\n",
    "\n",
    "train_length = x_train_obj.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_obj= torch.from_numpy(x_train_obj)\n",
    "y_train_obj=torch.from_numpy(y_train_obj)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train_obj, y_train_obj)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#fully connected layer after object detection\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(91, 28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = Net()\n",
    "\n",
    "result = model.train()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "genre_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
