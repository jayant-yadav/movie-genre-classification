{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "V0MlomrGAhhg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "from torch.nn.modules import MSELoss, L1Loss\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./Movie_Poster_Metadata/groundtruth\"\n",
    "temp_path = \"./Movie_Poster_Metadata/temp_groundtruth\"\n",
    "path2 = \"./Movie_Poster_Metadata/updated_groundtruth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input file and creating a clean one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(path1)\n",
    " \n",
    "if not os.path.exists(temp_path):\n",
    "  os.makedirs(temp_path)    \n",
    "\n",
    "if not os.path.exists(path2):\n",
    "  os.makedirs(path2)\n",
    "\n",
    "\n",
    "\n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(path1+'/'+file_name,'r',encoding='utf-16-le') as file1:\n",
    "\n",
    "        temp_file = open(temp_path+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        for line in file1.readlines():\n",
    "\n",
    "            line = line.replace(\"}\\n\",\"},\\n\")\n",
    "            \n",
    "            # reading all lines that begin with \"  \"_id\"\"\n",
    "            y = re.findall(\"^  \\\"_id\\\"\", line)\n",
    "            if not y:\n",
    "                temp_file.write(line)\n",
    "\n",
    "    file1.close()\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(temp_path)\n",
    " \n",
    "for file_name in dir_list:\n",
    "    \n",
    "    with open(temp_path+'/'+file_name,'r',encoding='utf-8') as temp_file:\n",
    "    \n",
    "        file2 = open(path2+'/'+file_name,'w',encoding='utf-8')\n",
    "\n",
    "        lines = temp_file.readlines()\n",
    "        lines = lines[1:-1]\n",
    "\n",
    "        file2.write(\"[{\")\n",
    "        file2.writelines(lines)\n",
    "        file2.write(\"}]\")\n",
    "        \n",
    "    temp_file.close()\n",
    "    file2.close()\n",
    "\n",
    "shutil.rmtree(temp_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to append all the json objects into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbID        object\n",
      "Director      object\n",
      "Genre         object\n",
      "imdbRating    object\n",
      "dtype: object\n",
      "       imdbID                                  Director  \\\n",
      "0   tt0080684                            Irvin Kershner   \n",
      "1   tt0081562                            Sidney Poitier   \n",
      "2   tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3   tt0080377                            Buddy Van Horn   \n",
      "4   tt0081375                              Howard Zieff   \n",
      "5   tt0080549                             Michael Apted   \n",
      "6   tt0081529                               Hal Needham   \n",
      "7   tt0080453                            Randal Kleiser   \n",
      "8   tt0080455                               John Landis   \n",
      "9   tt0081283                            Robert Redford   \n",
      "10  tt0081353                             Robert Altman   \n",
      "11  tt0081696                             James Bridges   \n",
      "12  tt0081505                           Stanley Kubrick   \n",
      "13  tt0081480                              Jay Sandrich   \n",
      "14  tt0080520                               Tommy Chong   \n",
      "15  tt0080487                              Harold Ramis   \n",
      "16  tt0080474                          Stuart Rosenberg   \n",
      "17  tt0081060                               Ron Maxwell   \n",
      "18  tt0080661                            Brian De Palma   \n",
      "19  tt0080948                         Richard Fleischer   \n",
      "\n",
      "                         Genre imdbRating  \n",
      "0   Action, Adventure, Fantasy        8.8  \n",
      "1                Comedy, Crime        6.8  \n",
      "2                       Comedy        7.8  \n",
      "3               Action, Comedy        6.0  \n",
      "4                  Comedy, War        6.1  \n",
      "5      Biography, Drama, Music        7.5  \n",
      "6               Action, Comedy        5.1  \n",
      "7    Adventure, Drama, Romance        5.7  \n",
      "8        Action, Comedy, Crime        7.9  \n",
      "9                        Drama        7.8  \n",
      "10   Adventure, Comedy, Family        5.2  \n",
      "11     Drama, Romance, Western        6.2  \n",
      "12               Drama, Horror        8.4  \n",
      "13             Comedy, Romance        6.7  \n",
      "14       Comedy, Crime, Sci-Fi        6.1  \n",
      "15               Comedy, Sport        7.4  \n",
      "16                       Drama        7.1  \n",
      "17               Comedy, Drama        6.4  \n",
      "18           Mystery, Thriller        7.1  \n",
      "19       Drama, Music, Romance        5.7  \n",
      "(8873, 4)\n"
     ]
    }
   ],
   "source": [
    "dir_list = os.listdir(path2)\n",
    "\n",
    "movies_df = pd.DataFrame()\n",
    "\n",
    "for file_name in dir_list:    \n",
    "\n",
    "#     try:\n",
    "    df = pd.read_json(path2+'/'+file_name,encoding='utf-8',orient='records')\n",
    "    df = df[['imdbID','Director','Genre','imdbRating']]\n",
    "    movies_df = pd.concat([movies_df,df], ignore_index=True)\n",
    "\n",
    "#     except:\n",
    "#         print(file_name)\n",
    "        \n",
    "print(movies_df.dtypes)\n",
    "print(movies_df.head(20))\n",
    "print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multi-hot encoded genre vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action' 'Comedy' 'Biography' 'Adventure' 'Drama' 'Mystery' 'Fantasy'\n",
      " 'Crime' 'Horror' 'Music' 'Western' 'Documentary' 'Family' 'Animation'\n",
      " 'Romance' 'Musical' 'Sci-Fi' 'Thriller' 'Short' 'War' 'Adult'\n",
      " 'Reality-TV' 'N/A' 'Game-Show' 'History' 'News' 'Sport' 'Talk-Show' None]\n",
      "      imdbID                                  Director  \\\n",
      "0  tt0080684                            Irvin Kershner   \n",
      "1  tt0081562                            Sidney Poitier   \n",
      "2  tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3  tt0080377                            Buddy Van Horn   \n",
      "4  tt0081375                              Howard Zieff   \n",
      "5  tt0080549                             Michael Apted   \n",
      "6  tt0081529                               Hal Needham   \n",
      "7  tt0080453                            Randal Kleiser   \n",
      "8  tt0080455                               John Landis   \n",
      "9  tt0081283                            Robert Redford   \n",
      "\n",
      "                        Genre imdbRating Genre0 Genre1 Genre2 Genre3  \n",
      "0  Action, Adventure, Fantasy        8.8      0      3      6   None  \n",
      "1               Comedy, Crime        6.8      1      7   None   None  \n",
      "2                      Comedy        7.8      1   None   None   None  \n",
      "3              Action, Comedy        6.0      0      1   None   None  \n",
      "4                 Comedy, War        6.1      1     19   None   None  \n",
      "5     Biography, Drama, Music        7.5      2      4      9   None  \n",
      "6              Action, Comedy        5.1      0      1   None   None  \n",
      "7   Adventure, Drama, Romance        5.7      3      4     14   None  \n",
      "8       Action, Comedy, Crime        7.9      0      1      7   None  \n",
      "9                       Drama        7.8      4   None   None   None  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_26176\\1996325303.py:7: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n",
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_26176\\1996325303.py:7: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n"
     ]
    }
   ],
   "source": [
    "#split genres in movies_df\n",
    "genres_df = movies_df['Genre'].str.split(', ', expand=True)\n",
    "genres_df.columns = ['Genre'+str(i) for i in genres_df.columns]\n",
    "movies_expanded_df = pd.concat([movies_df,genres_df], axis=1)\n",
    "\n",
    "#get unique genres\n",
    "unique_genres = (((movies_expanded_df[\"Genre0\"].append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).append(movies_expanded_df[\"Genre1\"])).unique()\n",
    "print(unique_genres)\n",
    "\n",
    "#replace genre strings with ids\n",
    "for genre in unique_genres:\n",
    "    if genre != None:\n",
    "        replacement = np.where(unique_genres == genre)[0][0]\n",
    "        \n",
    "        movies_expanded_df[\"Genre0\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre1\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre2\"].replace(genre, str(replacement), inplace=True)\n",
    "        movies_expanded_df[\"Genre3\"].replace(genre, str(replacement), inplace=True)\n",
    "print(movies_expanded_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovies_expanded_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenre1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m new_col_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m mlb\u001b[38;5;241m.\u001b[39mclasses_]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create new DataFrame with transformed/one-hot encoded IDs\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:755\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.fit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 755\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classes argument contains duplicate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses. Remove these duplicates before passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem to MultiLabelBinarizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    761\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(movies_expanded_df[\"Genre1\"])\n",
    "\n",
    "new_col_names = [\"ID_%s\" % c for c in mlb.classes_]\n",
    "\n",
    "# Create new DataFrame with transformed/one-hot encoded IDs\n",
    "ids = pd.DataFrame(mlb.fit_transform(movies_expanded_df[\"Genre1\"]), columns=new_col_names)\n",
    "\n",
    "# Concat with original \"Genre\" column\n",
    "genres_df = pd.concat( [movies_expanded_df, ids], axis=1 )\n",
    "\n",
    "print(genres_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = \"./Movie_Poster_Dataset\"\n",
    "\n",
    "# Going through all jpg-files, they are chopped up into 100x100 chunks and saved into a new folder\n",
    "for dirname in os.listdir(path3):\n",
    "    for filename in os.listdir(path3 + \"/\" + dirname):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if(ext == '.jpg'):\n",
    "            image = Image.open(os.path.join(path3 + \"/\" + dirname, filename))\n",
    "            width, height = image.size\n",
    "            chopsize = 100\n",
    "            for x0 in range(0, width, chopsize):\n",
    "                for y0 in range(0, height, chopsize):\n",
    "                    if(y0+chopsize <= height and x0+chopsize <= width):\n",
    "                        box = (x0, y0, x0+chopsize, y0+chopsize)\n",
    "                        image.crop(box).save('./Movie_Poster_Dataset_Cropped/%s.x%03d.y%03d.jpg' % (filename.replace('.jpg',''), x0, y0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through a convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training controls\n",
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "dropout = [0.3, 0.3, 0.3, 0.3, 0.2, 0.2, 0.2, 0.2, 0.15]\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8052\n",
      "8052\n",
      "      imdbID                                             Poster\n",
      "0  tt0079285  [[[64, 0, 4, 51, 28, 0, 8, 36, 108, 109, 113, ...\n",
      "1  tt0079302  [[[105, 117, 106, 100, 103, 99, 99, 97, 101, 9...\n",
      "2  tt0080339  [[[237, 238, 238, 238, 238, 238, 239, 239, 239...\n",
      "3  tt0080360  [[[84, 83, 84, 84, 87, 89, 93, 94, 93, 89, 88,...\n",
      "4  tt0080365  [[[162, 147, 139, 139, 138, 140, 140, 134, 132...\n",
      "5  tt0080377  [[[102, 93, 117, 85, 98, 96, 93, 101, 95, 92, ...\n",
      "6  tt0080402  [[[101, 112, 131, 91, 111, 120, 109, 111, 109,...\n",
      "7  tt0080437  [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\n",
      "8  tt0080442  [[[34, 39, 28, 27, 49, 47, 40, 51, 45, 39, 23,...\n",
      "9  tt0080453  [[[122, 129, 103, 64, 60, 40, 49, 56, 64, 67, ...\n"
     ]
    }
   ],
   "source": [
    "# the data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "flist=glob.glob('./Movie_Poster_Dataset/*/*.jpg')\n",
    "\n",
    "print(len(flist))\n",
    "\n",
    "imdb_id_arr = [\"0\" for a in range(len(flist))]\n",
    "image_arr = [\"0\" for a in range(len(flist))]\n",
    "index = 0\n",
    "\n",
    "for filename in flist:\n",
    "        \n",
    "    imdb_id = filename[filename.index(\"tt\"):filename.index(\".jpg\")]\n",
    "        \n",
    "    imdb_id_arr[index] = imdb_id\n",
    "                \n",
    "    img = np.array(cv2.imread(filename))\n",
    "    img = np.swapaxes(img, 2,0)\n",
    "    img = np.swapaxes(img, 2,1)\n",
    "    \n",
    "    image_arr[index] = img\n",
    "    \n",
    "    index +=1 \n",
    "    \n",
    "print(index)\n",
    "    \n",
    "image_dict = {\n",
    "    \"imdbID\": imdb_id_arr,\n",
    "    \"Poster\": image_arr\n",
    "}\n",
    "\n",
    "images_df = pd.DataFrame.from_dict(image_dict)#.astype({\"imdbID\": str, \"Poster\": np.float})\n",
    "\n",
    "print(images_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       imdbID                                  Director  \\\n",
      "0   tt0080684                            Irvin Kershner   \n",
      "1   tt0081562                            Sidney Poitier   \n",
      "2   tt0080339  Jim Abrahams, David Zucker, Jerry Zucker   \n",
      "3   tt0080377                            Buddy Van Horn   \n",
      "4   tt0081375                              Howard Zieff   \n",
      "5   tt0080549                             Michael Apted   \n",
      "6   tt0081529                               Hal Needham   \n",
      "7   tt0080453                            Randal Kleiser   \n",
      "8   tt0080455                               John Landis   \n",
      "9   tt0081283                            Robert Redford   \n",
      "10  tt0081353                             Robert Altman   \n",
      "11  tt0081696                             James Bridges   \n",
      "12  tt0081505                           Stanley Kubrick   \n",
      "13  tt0081480                              Jay Sandrich   \n",
      "14  tt0080520                               Tommy Chong   \n",
      "15  tt0080487                              Harold Ramis   \n",
      "16  tt0080474                          Stuart Rosenberg   \n",
      "17  tt0081060                               Ron Maxwell   \n",
      "18  tt0080661                            Brian De Palma   \n",
      "19  tt0080948                         Richard Fleischer   \n",
      "\n",
      "                         Genre imdbRating  \\\n",
      "0   Action, Adventure, Fantasy        8.8   \n",
      "1                Comedy, Crime        6.8   \n",
      "2                       Comedy        7.8   \n",
      "3               Action, Comedy        6.0   \n",
      "4                  Comedy, War        6.1   \n",
      "5      Biography, Drama, Music        7.5   \n",
      "6               Action, Comedy        5.1   \n",
      "7    Adventure, Drama, Romance        5.7   \n",
      "8        Action, Comedy, Crime        7.9   \n",
      "9                        Drama        7.8   \n",
      "10   Adventure, Comedy, Family        5.2   \n",
      "11     Drama, Romance, Western        6.2   \n",
      "12               Drama, Horror        8.4   \n",
      "13             Comedy, Romance        6.7   \n",
      "14       Comedy, Crime, Sci-Fi        6.1   \n",
      "15               Comedy, Sport        7.4   \n",
      "16                       Drama        7.1   \n",
      "17               Comedy, Drama        6.4   \n",
      "18           Mystery, Thriller        7.1   \n",
      "19       Drama, Music, Romance        5.7   \n",
      "\n",
      "                                               Poster  \n",
      "0   [[[255, 255, 255, 255, 255, 255, 255, 255, 254...  \n",
      "1   [[[253, 254, 255, 255, 255, 255, 255, 255, 254...  \n",
      "2   [[[237, 238, 238, 238, 238, 238, 239, 239, 239...  \n",
      "3   [[[102, 93, 117, 85, 98, 96, 93, 101, 95, 92, ...  \n",
      "4   [[[91, 92, 95, 96, 97, 98, 100, 101, 105, 103,...  \n",
      "5   [[[43, 43, 43, 43, 43, 42, 41, 41, 38, 37, 37,...  \n",
      "6   [[[98, 117, 88, 90, 96, 90, 79, 69, 68, 66, 67...  \n",
      "7   [[[122, 129, 103, 64, 60, 40, 49, 56, 64, 67, ...  \n",
      "8   [[[138, 138, 138, 138, 138, 138, 138, 138, 138...  \n",
      "9   [[[4, 4, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4...  \n",
      "10  [[[97, 84, 96, 87, 83, 80, 86, 82, 81, 81, 81,...  \n",
      "11  [[[17, 17, 16, 16, 16, 16, 16, 16, 17, 17, 17,...  \n",
      "12  [[[19, 10, 31, 13, 0, 1, 14, 28, 21, 14, 7, 6,...  \n",
      "13  [[[67, 65, 51, 57, 35, 36, 23, 36, 39, 41, 42,...  \n",
      "14  [[[252, 252, 252, 252, 252, 252, 252, 252, 252...  \n",
      "15  [[[230, 242, 242, 218, 228, 194, 98, 82, 64, 5...  \n",
      "16  [[[59, 84, 10, 7, 52, 42, 33, 8, 104, 124, 147...  \n",
      "17  [[[219, 219, 219, 219, 221, 221, 222, 222, 224...  \n",
      "18  [[[240, 239, 238, 238, 243, 243, 246, 246, 248...  \n",
      "19  [[[234, 162, 191, 186, 181, 185, 187, 180, 183...  \n"
     ]
    }
   ],
   "source": [
    "movies_df = pd.DataFrame.merge(movies_df, images_df, on=\"imdbID\", how=\"left\")\n",
    "print(movies_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data holders\n",
    "x_test = []\n",
    "x_train = []\n",
    "y_test= []\n",
    "y_train= []\n",
    "\n",
    "#the list of image files in SampleMoviePosters folder\n",
    "flist=glob.glob('./Movie_Poster_Dataset_Cropped/*.jpg')\n",
    "\n",
    "#setting the length of training data\n",
    "length=int(len(flist)*training_size)\n",
    "\n",
    "#currently using 1000 images\n",
    "length=1000*training_size\n",
    "\n",
    "#extracting the data about the images that are available\n",
    "i=0\n",
    "for filename in movie:\n",
    "    \n",
    "    imdb_id = filename[0:filename.index(\".\")]\n",
    "            \n",
    "    img = np.array(cv2.imread(filename))\n",
    "    img = np.swapaxes(img, 2,0)\n",
    "    img = np.swapaxes(img, 2,1)\n",
    "\n",
    "    if(i<length):\n",
    "        x_train.append(img)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        #idx = (genres_df.iloc[imdb_id])[Genre0]\n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_train.append(genre_arr)\n",
    "\n",
    "    else:\n",
    "        x_test.append(img)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        #idx = (genres_df.iloc[imdb_id])[Genre0]\n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_test.append(genre_arr)\n",
    "    i+=1\n",
    "    #stopping after 1000 images\n",
    "    if i==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (700, 3, 100, 100)\n",
      "700 train samples\n",
      "300 test samples\n"
     ]
    }
   ],
   "source": [
    "#converting the data from lists to numpy arrays\n",
    "x_train=np.asarray(x_train,dtype=float)\n",
    "x_test=np.asarray(x_test,dtype=float)\n",
    "y_train=np.asarray(y_train,dtype=float)\n",
    "y_test=np.asarray(y_test,dtype=float)\n",
    "\n",
    "#scaling down the RGB data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "train_length = x_train.shape[0]\n",
    "\n",
    "x_train=torch.from_numpy(x_train)\n",
    "x_test=torch.from_numpy(x_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(x_test, y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m targets \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(movies_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m le \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mLabelEncoder()\n\u001b[1;32m----> 5\u001b[0m movies_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmovies_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m targets \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(movies_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m le \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mLabelEncoder()\n\u001b[1;32m----> 5\u001b[0m movies_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m movies_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1038\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1030\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1035\u001b[0m         )\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1040\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "#le = sklearn.preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(movies_df[\"Genre\"])\n",
    "\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "movies_df['target'] = movies_df.apply(lambda x: le.fit_transform(x[\"Genre\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Poster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Poster'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices\n\u001b[0;32m      3\u001b[0m     (\n\u001b[0;32m      4\u001b[0m         (\n\u001b[1;32m----> 5\u001b[0m             torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mmovies_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPoster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues),\n\u001b[0;32m      6\u001b[0m             targets\n\u001b[0;32m      7\u001b[0m         )\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Poster'"
     ]
    }
   ],
   "source": [
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices\n",
    "    (\n",
    "        (\n",
    "            torch.from_numpy(movies_df[\"Poster\"].values),\n",
    "            targets\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv1_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv2): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4_drop): Dropout2d(p=0.3, inplace=False)\n",
      "  (conv5): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv5_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (conv6): Conv2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv6_drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3_drop): Dropout(p=0.15, inplace=False)\n",
      "  (fc4): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\carol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([20, 28])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/700 (0%)]\tLoss: 0.040060 0.074887\n",
      "Train Epoch: 0 [20/700 (3%)]\tLoss: 0.039105 0.068092\n",
      "Train Epoch: 0 [40/700 (6%)]\tLoss: 0.037542 0.057164\n",
      "Train Epoch: 0 [60/700 (9%)]\tLoss: 0.038051 0.075392\n",
      "Train Epoch: 0 [80/700 (11%)]\tLoss: 0.036772 0.049336\n",
      "Train Epoch: 0 [100/700 (14%)]\tLoss: 0.035193 0.054444\n",
      "Train Epoch: 0 [120/700 (17%)]\tLoss: 0.034795 0.065989\n",
      "Train Epoch: 0 [140/700 (20%)]\tLoss: 0.035384 0.065460\n",
      "Train Epoch: 0 [160/700 (23%)]\tLoss: 0.035779 0.057114\n",
      "Train Epoch: 0 [180/700 (26%)]\tLoss: 0.035289 0.058904\n",
      "Train Epoch: 0 [200/700 (29%)]\tLoss: 0.035361 0.058584\n",
      "Train Epoch: 0 [220/700 (31%)]\tLoss: 0.035739 0.063090\n",
      "Train Epoch: 0 [240/700 (34%)]\tLoss: 0.035427 0.057253\n",
      "Train Epoch: 0 [260/700 (37%)]\tLoss: 0.035181 0.063147\n",
      "Train Epoch: 0 [280/700 (40%)]\tLoss: 0.035759 0.064822\n",
      "Train Epoch: 0 [300/700 (43%)]\tLoss: 0.036004 0.059184\n",
      "Train Epoch: 0 [320/700 (46%)]\tLoss: 0.034960 0.059980\n",
      "Train Epoch: 0 [340/700 (49%)]\tLoss: 0.035042 0.059134\n",
      "Train Epoch: 0 [360/700 (51%)]\tLoss: 0.035422 0.063183\n",
      "Train Epoch: 0 [380/700 (54%)]\tLoss: 0.035165 0.062907\n",
      "Train Epoch: 0 [400/700 (57%)]\tLoss: 0.035212 0.062266\n",
      "Train Epoch: 0 [420/700 (60%)]\tLoss: 0.034894 0.064456\n",
      "Train Epoch: 0 [440/700 (63%)]\tLoss: 0.034636 0.067013\n",
      "Train Epoch: 0 [460/700 (66%)]\tLoss: 0.035307 0.061053\n",
      "Train Epoch: 0 [480/700 (69%)]\tLoss: 0.035062 0.068112\n",
      "Train Epoch: 0 [500/700 (71%)]\tLoss: 0.035247 0.062497\n",
      "Train Epoch: 0 [520/700 (74%)]\tLoss: 0.034855 0.072634\n",
      "Train Epoch: 0 [540/700 (77%)]\tLoss: 0.035108 0.057879\n",
      "Train Epoch: 0 [560/700 (80%)]\tLoss: 0.034631 0.064715\n",
      "Train Epoch: 0 [580/700 (83%)]\tLoss: 0.035110 0.061581\n",
      "Train Epoch: 0 [600/700 (86%)]\tLoss: 0.034821 0.064617\n",
      "Train Epoch: 0 [620/700 (89%)]\tLoss: 0.034933 0.066258\n",
      "Train Epoch: 0 [640/700 (91%)]\tLoss: 0.034829 0.065831\n",
      "Train Epoch: 0 [660/700 (94%)]\tLoss: 0.035166 0.056629\n",
      "Train Epoch: 0 [680/700 (97%)]\tLoss: 0.034786 0.067360\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_84916\\559374431.py:86: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True).float(), Variable(target).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0344 \n",
      "Average abs_loss: 0.0672 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n",
      "Train Epoch: 1 [0/700 (0%)]\tLoss: 0.034965 0.064767\n",
      "Train Epoch: 1 [20/700 (3%)]\tLoss: 0.034955 0.069272\n",
      "Train Epoch: 1 [40/700 (6%)]\tLoss: 0.034777 0.064887\n",
      "Train Epoch: 1 [60/700 (9%)]\tLoss: 0.034708 0.070597\n",
      "Train Epoch: 1 [80/700 (11%)]\tLoss: 0.034495 0.070145\n",
      "Train Epoch: 1 [100/700 (14%)]\tLoss: 0.034734 0.064066\n",
      "Train Epoch: 1 [120/700 (17%)]\tLoss: 0.034731 0.064574\n",
      "Train Epoch: 1 [140/700 (20%)]\tLoss: 0.034830 0.064226\n",
      "Train Epoch: 1 [160/700 (23%)]\tLoss: 0.034589 0.069115\n",
      "Train Epoch: 1 [180/700 (26%)]\tLoss: 0.034868 0.062671\n",
      "Train Epoch: 1 [200/700 (29%)]\tLoss: 0.034700 0.064548\n",
      "Train Epoch: 1 [220/700 (31%)]\tLoss: 0.034725 0.069050\n",
      "Train Epoch: 1 [240/700 (34%)]\tLoss: 0.034626 0.067209\n",
      "Train Epoch: 1 [260/700 (37%)]\tLoss: 0.034780 0.064893\n",
      "Train Epoch: 1 [280/700 (40%)]\tLoss: 0.034563 0.067498\n",
      "Train Epoch: 1 [300/700 (43%)]\tLoss: 0.034598 0.064039\n",
      "Train Epoch: 1 [320/700 (46%)]\tLoss: 0.034653 0.064781\n",
      "Train Epoch: 1 [340/700 (49%)]\tLoss: 0.034784 0.061880\n",
      "Train Epoch: 1 [360/700 (51%)]\tLoss: 0.034651 0.065339\n",
      "Train Epoch: 1 [380/700 (54%)]\tLoss: 0.034749 0.064056\n",
      "Train Epoch: 1 [400/700 (57%)]\tLoss: 0.034904 0.057272\n",
      "Train Epoch: 1 [420/700 (60%)]\tLoss: 0.034460 0.068884\n",
      "Train Epoch: 1 [440/700 (63%)]\tLoss: 0.034606 0.065530\n",
      "Train Epoch: 1 [460/700 (66%)]\tLoss: 0.034467 0.071342\n",
      "Train Epoch: 1 [480/700 (69%)]\tLoss: 0.034540 0.065883\n",
      "Train Epoch: 1 [500/700 (71%)]\tLoss: 0.034694 0.066003\n",
      "Train Epoch: 1 [520/700 (74%)]\tLoss: 0.034683 0.063197\n",
      "Train Epoch: 1 [540/700 (77%)]\tLoss: 0.034646 0.065902\n",
      "Train Epoch: 1 [560/700 (80%)]\tLoss: 0.034640 0.063762\n",
      "Train Epoch: 1 [580/700 (83%)]\tLoss: 0.034639 0.065965\n",
      "Train Epoch: 1 [600/700 (86%)]\tLoss: 0.034575 0.064985\n",
      "Train Epoch: 1 [620/700 (89%)]\tLoss: 0.034529 0.071396\n",
      "Train Epoch: 1 [640/700 (91%)]\tLoss: 0.034554 0.066290\n",
      "Train Epoch: 1 [660/700 (94%)]\tLoss: 0.034529 0.064018\n",
      "Train Epoch: 1 [680/700 (97%)]\tLoss: 0.034613 0.064369\n",
      "test\n",
      "\n",
      "Test set: \n",
      "Average sq_loss: 0.0344 \n",
      "Average abs_loss: 0.0705 \n",
      "Guessed 100% correct: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape=(3, img_rows, img_cols)):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=2)\n",
    "        self.conv1_drop = nn.Dropout2d(p=dropout[0])\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=2)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout[1])\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv3_drop = nn.Dropout2d(p=dropout[2])\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2)\n",
    "        self.conv4_drop = nn.Dropout2d(p=dropout[3])\n",
    "        self.conv5 = nn.Conv2d(64, 32, kernel_size=2)\n",
    "        self.conv5_drop = nn.Dropout2d(p=dropout[4])\n",
    "        self.conv6 = nn.Conv2d(32, 16, kernel_size=2)\n",
    "        self.conv6_drop = nn.Dropout2d(p=dropout[5])\n",
    "        \n",
    "        n_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_size, 16)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout[6])\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc2_drop = nn.Dropout(p=dropout[7])\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc3_drop = nn.Dropout(p=dropout[8])\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        \n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = Variable(torch.rand(bs, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv4_drop(self.conv4(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv5_drop(self.conv5(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv6_drop(self.conv6(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.relu(self.fc2_drop(self.fc2(x)))\n",
    "        x = F.relu(self.fc3_drop(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        human_loss= human_criterion(output, target)\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data, human_loss.data))\n",
    "\n",
    "def test():\n",
    "    print('test')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    human_loss = 0\n",
    "    i = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        i+=1\n",
    "        data, target = Variable(data, volatile=True).float(), Variable(target).float()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "        human_loss += human_criterion(output, target)\n",
    "        if loss==0:\n",
    "            correct+=1\n",
    "\n",
    "    print('\\nTest set: \\nAverage sq_loss: {:.4f} \\nAverage abs_loss: {:.4f} \\nGuessed 100% correct: {:.4f}\\n'.format(test_loss.data/i, human_loss.data/i, correct))\n",
    "\n",
    "model.float()\n",
    "print(model)\n",
    "for epoch in range(0, epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the images through object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 2\n",
    "training_size = 0.7\n",
    "learning_rate = 0.001\n",
    "img_rows, img_cols = 100, 100\n",
    "\n",
    "x_test_obj = []\n",
    "x_train_obj = []\n",
    "y_test_obj= []\n",
    "y_train_obj= []\n",
    "tempY_obj = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Movie_Poster_Dataset_Cropped\"\n",
    "\n",
    "#pretrained YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "counter = 1000\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if(ext == '.jpg'):\n",
    "        image = Image.open(os.path.join(path, filename))\n",
    "        results = model(image, size = 100)\n",
    "            \n",
    "        #create an array for the 91 object categories and set initial confidence to 0\n",
    "        obj_arr = np.empty([91])\n",
    "        for x in range(91):\n",
    "            obj_arr[x] = 0.0\n",
    "\n",
    "        #update the confidence values according to the object detection results\n",
    "        for obj in results.pandas().xyxy[0]:\n",
    "            index =  results.pandas().xyxy[0]['class']\n",
    "            obj_arr[index] = obj_arr[index] + results.pandas().xyxy[0].confidence\n",
    "        \n",
    "        x_train_obj.append(obj_arr)\n",
    "        \n",
    "        #generating a random genre array (desired output)\n",
    "        genre_arr = np.empty([28])\n",
    "    \n",
    "        for x in range(28):\n",
    "            genre_arr[x] = 0\n",
    "        \n",
    "        rnd = random.randint(0, 27)\n",
    "        genre_arr[rnd] = 1\n",
    "        y_train_obj.append(genre_arr)\n",
    "        \n",
    "        counter -= 1\n",
    "        if counter == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1000, 91)\n",
      "1000 train samples\n"
     ]
    }
   ],
   "source": [
    "x_train_obj = np.asarray(x_train_obj, dtype=float)\n",
    "y_train_obj = np.asarray(y_train_obj, dtype=float)\n",
    "\n",
    "#printing stats about the features\n",
    "print('x_train shape:', x_train_obj.shape)\n",
    "print(x_train_obj.shape[0], 'train samples')\n",
    "\n",
    "train_length = x_train_obj.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_obj= torch.from_numpy(x_train_obj)\n",
    "y_train_obj=torch.from_numpy(y_train_obj)\n",
    "\n",
    "train = data_utils.TensorDataset(x_train_obj, y_train_obj)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully connected layer after object detection\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(91, 28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = Net()\n",
    "\n",
    "result = model.train()\n",
    "criterion = MSELoss(size_average=True)\n",
    "human_criterion = L1Loss(size_average=True)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,\n",
    "            alpha=0.9, eps=1e-08, weight_decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "genre_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
